== Physical Plan ==
TakeOrderedAndProject (59)
+- * Project (58)
   +- * BroadcastHashJoin Inner BuildRight (57)
      :- * Project (42)
      :  +- * BroadcastHashJoin Inner BuildRight (41)
      :     :- * Filter (26)
      :     :  +- * HashAggregate (25)
      :     :     +- Exchange (24)
      :     :        +- * HashAggregate (23)
      :     :           +- * Project (22)
      :     :              +- * BroadcastHashJoin Inner BuildRight (21)
      :     :                 :- * Project (9)
      :     :                 :  +- * BroadcastHashJoin Inner BuildRight (8)
      :     :                 :     :- * Filter (3)
      :     :                 :     :  +- * ColumnarToRow (2)
      :     :                 :     :     +- Scan parquet spark_catalog.default.store_sales (1)
      :     :                 :     +- BroadcastExchange (7)
      :     :                 :        +- * Filter (6)
      :     :                 :           +- * ColumnarToRow (5)
      :     :                 :              +- Scan parquet spark_catalog.default.item (4)
      :     :                 +- BroadcastExchange (20)
      :     :                    +- * Project (19)
      :     :                       +- * BroadcastHashJoin LeftSemi BuildRight (18)
      :     :                          :- * Filter (12)
      :     :                          :  +- * ColumnarToRow (11)
      :     :                          :     +- Scan parquet spark_catalog.default.date_dim (10)
      :     :                          +- BroadcastExchange (17)
      :     :                             +- * Project (16)
      :     :                                +- * Filter (15)
      :     :                                   +- * ColumnarToRow (14)
      :     :                                      +- Scan parquet spark_catalog.default.date_dim (13)
      :     +- BroadcastExchange (40)
      :        +- * Filter (39)
      :           +- * HashAggregate (38)
      :              +- Exchange (37)
      :                 +- * HashAggregate (36)
      :                    +- * Project (35)
      :                       +- * BroadcastHashJoin Inner BuildRight (34)
      :                          :- * Project (32)
      :                          :  +- * BroadcastHashJoin Inner BuildRight (31)
      :                          :     :- * Filter (29)
      :                          :     :  +- * ColumnarToRow (28)
      :                          :     :     +- Scan parquet spark_catalog.default.catalog_sales (27)
      :                          :     +- ReusedExchange (30)
      :                          +- ReusedExchange (33)
      +- BroadcastExchange (56)
         +- * Filter (55)
            +- * HashAggregate (54)
               +- Exchange (53)
                  +- * HashAggregate (52)
                     +- * Project (51)
                        +- * BroadcastHashJoin Inner BuildRight (50)
                           :- * Project (48)
                           :  +- * BroadcastHashJoin Inner BuildRight (47)
                           :     :- * Filter (45)
                           :     :  +- * ColumnarToRow (44)
                           :     :     +- Scan parquet spark_catalog.default.web_sales (43)
                           :     +- ReusedExchange (46)
                           +- ReusedExchange (49)


(1) Scan parquet spark_catalog.default.store_sales
Output [3]: [ss_item_sk#1, ss_ext_sales_price#2, ss_sold_date_sk#3]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#3)]
PushedFilters: [IsNotNull(ss_item_sk)]
ReadSchema: struct<ss_item_sk:int,ss_ext_sales_price:decimal(7,2)>

(2) ColumnarToRow [codegen id : 4]
Input [3]: [ss_item_sk#1, ss_ext_sales_price#2, ss_sold_date_sk#3]

(3) Filter [codegen id : 4]
Input [3]: [ss_item_sk#1, ss_ext_sales_price#2, ss_sold_date_sk#3]
Condition : isnotnull(ss_item_sk#1)

(4) Scan parquet spark_catalog.default.item
Output [2]: [i_item_sk#4, i_item_id#5]
Batched: true
Location [not included in comparison]/{warehouse_dir}/item]
PushedFilters: [IsNotNull(i_item_sk), IsNotNull(i_item_id)]
ReadSchema: struct<i_item_sk:int,i_item_id:string>

(5) ColumnarToRow [codegen id : 1]
Input [2]: [i_item_sk#4, i_item_id#5]

(6) Filter [codegen id : 1]
Input [2]: [i_item_sk#4, i_item_id#5]
Condition : (isnotnull(i_item_sk#4) AND isnotnull(i_item_id#5))

(7) BroadcastExchange
Input [2]: [i_item_sk#4, i_item_id#5]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=1]

(8) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_item_sk#1]
Right keys [1]: [i_item_sk#4]
Join type: Inner
Join condition: None

(9) Project [codegen id : 4]
Output [3]: [ss_ext_sales_price#2, ss_sold_date_sk#3, i_item_id#5]
Input [5]: [ss_item_sk#1, ss_ext_sales_price#2, ss_sold_date_sk#3, i_item_sk#4, i_item_id#5]

(10) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#6, d_date#7]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_date:date>

(11) ColumnarToRow [codegen id : 3]
Input [2]: [d_date_sk#6, d_date#7]

(12) Filter [codegen id : 3]
Input [2]: [d_date_sk#6, d_date#7]
Condition : isnotnull(d_date_sk#6)

(13) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date#8, d_week_seq#9]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_week_seq)]
ReadSchema: struct<d_date:date,d_week_seq:int>

(14) ColumnarToRow [codegen id : 2]
Input [2]: [d_date#8, d_week_seq#9]

(15) Filter [codegen id : 2]
Input [2]: [d_date#8, d_week_seq#9]
Condition : (isnotnull(d_week_seq#9) AND (d_week_seq#9 = Subquery scalar-subquery#10, [id=#11]))

(16) Project [codegen id : 2]
Output [1]: [d_date#8]
Input [2]: [d_date#8, d_week_seq#9]

(17) BroadcastExchange
Input [1]: [d_date#8]
Arguments: HashedRelationBroadcastMode(List(input[0, date, true]),false), [plan_id=2]

(18) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [d_date#7]
Right keys [1]: [d_date#8]
Join type: LeftSemi
Join condition: None

(19) Project [codegen id : 3]
Output [1]: [d_date_sk#6]
Input [2]: [d_date_sk#6, d_date#7]

(20) BroadcastExchange
Input [1]: [d_date_sk#6]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=3]

(21) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_sold_date_sk#3]
Right keys [1]: [d_date_sk#6]
Join type: Inner
Join condition: None

(22) Project [codegen id : 4]
Output [2]: [ss_ext_sales_price#2, i_item_id#5]
Input [4]: [ss_ext_sales_price#2, ss_sold_date_sk#3, i_item_id#5, d_date_sk#6]

(23) HashAggregate [codegen id : 4]
Input [2]: [ss_ext_sales_price#2, i_item_id#5]
Keys [1]: [i_item_id#5]
Functions [1]: [partial_sum(UnscaledValue(ss_ext_sales_price#2))]
Aggregate Attributes [1]: [sum#12]
Results [2]: [i_item_id#5, sum#13]

(24) Exchange
Input [2]: [i_item_id#5, sum#13]
Arguments: hashpartitioning(i_item_id#5, 5), ENSURE_REQUIREMENTS, [plan_id=4]

(25) HashAggregate [codegen id : 15]
Input [2]: [i_item_id#5, sum#13]
Keys [1]: [i_item_id#5]
Functions [1]: [sum(UnscaledValue(ss_ext_sales_price#2))]
Aggregate Attributes [1]: [sum(UnscaledValue(ss_ext_sales_price#2))#14]
Results [2]: [i_item_id#5 AS item_id#15, MakeDecimal(sum(UnscaledValue(ss_ext_sales_price#2))#14,17,2) AS ss_item_rev#16]

(26) Filter [codegen id : 15]
Input [2]: [item_id#15, ss_item_rev#16]
Condition : isnotnull(ss_item_rev#16)

(27) Scan parquet spark_catalog.default.catalog_sales
Output [3]: [cs_item_sk#17, cs_ext_sales_price#18, cs_sold_date_sk#19]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#19)]
PushedFilters: [IsNotNull(cs_item_sk)]
ReadSchema: struct<cs_item_sk:int,cs_ext_sales_price:decimal(7,2)>

(28) ColumnarToRow [codegen id : 8]
Input [3]: [cs_item_sk#17, cs_ext_sales_price#18, cs_sold_date_sk#19]

(29) Filter [codegen id : 8]
Input [3]: [cs_item_sk#17, cs_ext_sales_price#18, cs_sold_date_sk#19]
Condition : isnotnull(cs_item_sk#17)

(30) ReusedExchange [Reuses operator id: 7]
Output [2]: [i_item_sk#20, i_item_id#21]

(31) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [cs_item_sk#17]
Right keys [1]: [i_item_sk#20]
Join type: Inner
Join condition: None

(32) Project [codegen id : 8]
Output [3]: [cs_ext_sales_price#18, cs_sold_date_sk#19, i_item_id#21]
Input [5]: [cs_item_sk#17, cs_ext_sales_price#18, cs_sold_date_sk#19, i_item_sk#20, i_item_id#21]

(33) ReusedExchange [Reuses operator id: 20]
Output [1]: [d_date_sk#22]

(34) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [cs_sold_date_sk#19]
Right keys [1]: [d_date_sk#22]
Join type: Inner
Join condition: None

(35) Project [codegen id : 8]
Output [2]: [cs_ext_sales_price#18, i_item_id#21]
Input [4]: [cs_ext_sales_price#18, cs_sold_date_sk#19, i_item_id#21, d_date_sk#22]

(36) HashAggregate [codegen id : 8]
Input [2]: [cs_ext_sales_price#18, i_item_id#21]
Keys [1]: [i_item_id#21]
Functions [1]: [partial_sum(UnscaledValue(cs_ext_sales_price#18))]
Aggregate Attributes [1]: [sum#23]
Results [2]: [i_item_id#21, sum#24]

(37) Exchange
Input [2]: [i_item_id#21, sum#24]
Arguments: hashpartitioning(i_item_id#21, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(38) HashAggregate [codegen id : 9]
Input [2]: [i_item_id#21, sum#24]
Keys [1]: [i_item_id#21]
Functions [1]: [sum(UnscaledValue(cs_ext_sales_price#18))]
Aggregate Attributes [1]: [sum(UnscaledValue(cs_ext_sales_price#18))#25]
Results [2]: [i_item_id#21 AS item_id#26, MakeDecimal(sum(UnscaledValue(cs_ext_sales_price#18))#25,17,2) AS cs_item_rev#27]

(39) Filter [codegen id : 9]
Input [2]: [item_id#26, cs_item_rev#27]
Condition : isnotnull(cs_item_rev#27)

(40) BroadcastExchange
Input [2]: [item_id#26, cs_item_rev#27]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=6]

(41) BroadcastHashJoin [codegen id : 15]
Left keys [1]: [item_id#15]
Right keys [1]: [item_id#26]
Join type: Inner
Join condition: ((((cast(ss_item_rev#16 as decimal(19,3)) >= (0.9 * cs_item_rev#27)) AND (cast(ss_item_rev#16 as decimal(20,3)) <= (1.1 * cs_item_rev#27))) AND (cast(cs_item_rev#27 as decimal(19,3)) >= (0.9 * ss_item_rev#16))) AND (cast(cs_item_rev#27 as decimal(20,3)) <= (1.1 * ss_item_rev#16)))

(42) Project [codegen id : 15]
Output [3]: [item_id#15, ss_item_rev#16, cs_item_rev#27]
Input [4]: [item_id#15, ss_item_rev#16, item_id#26, cs_item_rev#27]

(43) Scan parquet spark_catalog.default.web_sales
Output [3]: [ws_item_sk#28, ws_ext_sales_price#29, ws_sold_date_sk#30]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ws_sold_date_sk#30)]
PushedFilters: [IsNotNull(ws_item_sk)]
ReadSchema: struct<ws_item_sk:int,ws_ext_sales_price:decimal(7,2)>

(44) ColumnarToRow [codegen id : 13]
Input [3]: [ws_item_sk#28, ws_ext_sales_price#29, ws_sold_date_sk#30]

(45) Filter [codegen id : 13]
Input [3]: [ws_item_sk#28, ws_ext_sales_price#29, ws_sold_date_sk#30]
Condition : isnotnull(ws_item_sk#28)

(46) ReusedExchange [Reuses operator id: 7]
Output [2]: [i_item_sk#31, i_item_id#32]

(47) BroadcastHashJoin [codegen id : 13]
Left keys [1]: [ws_item_sk#28]
Right keys [1]: [i_item_sk#31]
Join type: Inner
Join condition: None

(48) Project [codegen id : 13]
Output [3]: [ws_ext_sales_price#29, ws_sold_date_sk#30, i_item_id#32]
Input [5]: [ws_item_sk#28, ws_ext_sales_price#29, ws_sold_date_sk#30, i_item_sk#31, i_item_id#32]

(49) ReusedExchange [Reuses operator id: 20]
Output [1]: [d_date_sk#33]

(50) BroadcastHashJoin [codegen id : 13]
Left keys [1]: [ws_sold_date_sk#30]
Right keys [1]: [d_date_sk#33]
Join type: Inner
Join condition: None

(51) Project [codegen id : 13]
Output [2]: [ws_ext_sales_price#29, i_item_id#32]
Input [4]: [ws_ext_sales_price#29, ws_sold_date_sk#30, i_item_id#32, d_date_sk#33]

(52) HashAggregate [codegen id : 13]
Input [2]: [ws_ext_sales_price#29, i_item_id#32]
Keys [1]: [i_item_id#32]
Functions [1]: [partial_sum(UnscaledValue(ws_ext_sales_price#29))]
Aggregate Attributes [1]: [sum#34]
Results [2]: [i_item_id#32, sum#35]

(53) Exchange
Input [2]: [i_item_id#32, sum#35]
Arguments: hashpartitioning(i_item_id#32, 5), ENSURE_REQUIREMENTS, [plan_id=7]

(54) HashAggregate [codegen id : 14]
Input [2]: [i_item_id#32, sum#35]
Keys [1]: [i_item_id#32]
Functions [1]: [sum(UnscaledValue(ws_ext_sales_price#29))]
Aggregate Attributes [1]: [sum(UnscaledValue(ws_ext_sales_price#29))#36]
Results [2]: [i_item_id#32 AS item_id#37, MakeDecimal(sum(UnscaledValue(ws_ext_sales_price#29))#36,17,2) AS ws_item_rev#38]

(55) Filter [codegen id : 14]
Input [2]: [item_id#37, ws_item_rev#38]
Condition : isnotnull(ws_item_rev#38)

(56) BroadcastExchange
Input [2]: [item_id#37, ws_item_rev#38]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=8]

(57) BroadcastHashJoin [codegen id : 15]
Left keys [1]: [item_id#15]
Right keys [1]: [item_id#37]
Join type: Inner
Join condition: ((((((((cast(ss_item_rev#16 as decimal(19,3)) >= (0.9 * ws_item_rev#38)) AND (cast(ss_item_rev#16 as decimal(20,3)) <= (1.1 * ws_item_rev#38))) AND (cast(cs_item_rev#27 as decimal(19,3)) >= (0.9 * ws_item_rev#38))) AND (cast(cs_item_rev#27 as decimal(20,3)) <= (1.1 * ws_item_rev#38))) AND (cast(ws_item_rev#38 as decimal(19,3)) >= (0.9 * ss_item_rev#16))) AND (cast(ws_item_rev#38 as decimal(20,3)) <= (1.1 * ss_item_rev#16))) AND (cast(ws_item_rev#38 as decimal(19,3)) >= (0.9 * cs_item_rev#27))) AND (cast(ws_item_rev#38 as decimal(20,3)) <= (1.1 * cs_item_rev#27)))

(58) Project [codegen id : 15]
Output [8]: [item_id#15, ss_item_rev#16, (((ss_item_rev#16 / ((ss_item_rev#16 + cs_item_rev#27) + ws_item_rev#38)) / 3) * 100) AS ss_dev#39, cs_item_rev#27, (((cs_item_rev#27 / ((ss_item_rev#16 + cs_item_rev#27) + ws_item_rev#38)) / 3) * 100) AS cs_dev#40, ws_item_rev#38, (((ws_item_rev#38 / ((ss_item_rev#16 + cs_item_rev#27) + ws_item_rev#38)) / 3) * 100) AS ws_dev#41, (((ss_item_rev#16 + cs_item_rev#27) + ws_item_rev#38) / 3) AS average#42]
Input [5]: [item_id#15, ss_item_rev#16, cs_item_rev#27, item_id#37, ws_item_rev#38]

(59) TakeOrderedAndProject
Input [8]: [item_id#15, ss_item_rev#16, ss_dev#39, cs_item_rev#27, cs_dev#40, ws_item_rev#38, ws_dev#41, average#42]
Arguments: 100, [item_id#15 ASC NULLS FIRST, ss_item_rev#16 ASC NULLS FIRST], [item_id#15, ss_item_rev#16, ss_dev#39, cs_item_rev#27, cs_dev#40, ws_item_rev#38, ws_dev#41, average#42]

===== Subqueries =====

Subquery:1 Hosting operator id = 15 Hosting Expression = Subquery scalar-subquery#10, [id=#11]
* Project (63)
+- * Filter (62)
   +- * ColumnarToRow (61)
      +- Scan parquet spark_catalog.default.date_dim (60)


(60) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date#43, d_week_seq#44]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_date), EqualTo(d_date,2000-01-03)]
ReadSchema: struct<d_date:date,d_week_seq:int>

(61) ColumnarToRow [codegen id : 1]
Input [2]: [d_date#43, d_week_seq#44]

(62) Filter [codegen id : 1]
Input [2]: [d_date#43, d_week_seq#44]
Condition : (isnotnull(d_date#43) AND (d_date#43 = 2000-01-03))

(63) Project [codegen id : 1]
Output [1]: [d_week_seq#44]
Input [2]: [d_date#43, d_week_seq#44]


