== Physical Plan ==
* HashAggregate (74)
+- Exchange (73)
   +- * HashAggregate (72)
      +- Union (71)
         :- * Project (53)
         :  +- * BroadcastHashJoin Inner BuildRight (52)
         :     :- * Project (46)
         :     :  +- * SortMergeJoin LeftSemi (45)
         :     :     :- * Sort (28)
         :     :     :  +- Exchange (27)
         :     :     :     +- * Project (26)
         :     :     :        +- * BroadcastHashJoin LeftSemi BuildRight (25)
         :     :     :           :- * ColumnarToRow (2)
         :     :     :           :  +- Scan parquet spark_catalog.default.catalog_sales (1)
         :     :     :           +- BroadcastExchange (24)
         :     :     :              +- * Project (23)
         :     :     :                 +- * Filter (22)
         :     :     :                    +- * HashAggregate (21)
         :     :     :                       +- Exchange (20)
         :     :     :                          +- * HashAggregate (19)
         :     :     :                             +- * Project (18)
         :     :     :                                +- * BroadcastHashJoin Inner BuildRight (17)
         :     :     :                                   :- * Project (12)
         :     :     :                                   :  +- * BroadcastHashJoin Inner BuildRight (11)
         :     :     :                                   :     :- * Filter (5)
         :     :     :                                   :     :  +- * ColumnarToRow (4)
         :     :     :                                   :     :     +- Scan parquet spark_catalog.default.store_sales (3)
         :     :     :                                   :     +- BroadcastExchange (10)
         :     :     :                                   :        +- * Project (9)
         :     :     :                                   :           +- * Filter (8)
         :     :     :                                   :              +- * ColumnarToRow (7)
         :     :     :                                   :                 +- Scan parquet spark_catalog.default.date_dim (6)
         :     :     :                                   +- BroadcastExchange (16)
         :     :     :                                      +- * Filter (15)
         :     :     :                                         +- * ColumnarToRow (14)
         :     :     :                                            +- Scan parquet spark_catalog.default.item (13)
         :     :     +- * Sort (44)
         :     :        +- * Project (43)
         :     :           +- * Filter (42)
         :     :              +- * HashAggregate (41)
         :     :                 +- Exchange (40)
         :     :                    +- * HashAggregate (39)
         :     :                       +- * Project (38)
         :     :                          +- * BroadcastHashJoin Inner BuildRight (37)
         :     :                             :- * Project (32)
         :     :                             :  +- * Filter (31)
         :     :                             :     +- * ColumnarToRow (30)
         :     :                             :        +- Scan parquet spark_catalog.default.store_sales (29)
         :     :                             +- BroadcastExchange (36)
         :     :                                +- * Filter (35)
         :     :                                   +- * ColumnarToRow (34)
         :     :                                      +- Scan parquet spark_catalog.default.customer (33)
         :     +- BroadcastExchange (51)
         :        +- * Project (50)
         :           +- * Filter (49)
         :              +- * ColumnarToRow (48)
         :                 +- Scan parquet spark_catalog.default.date_dim (47)
         +- * Project (70)
            +- * BroadcastHashJoin Inner BuildRight (69)
               :- * Project (67)
               :  +- * SortMergeJoin LeftSemi (66)
               :     :- * Sort (60)
               :     :  +- Exchange (59)
               :     :     +- * Project (58)
               :     :        +- * BroadcastHashJoin LeftSemi BuildRight (57)
               :     :           :- * ColumnarToRow (55)
               :     :           :  +- Scan parquet spark_catalog.default.web_sales (54)
               :     :           +- ReusedExchange (56)
               :     +- * Sort (65)
               :        +- * Project (64)
               :           +- * Filter (63)
               :              +- * HashAggregate (62)
               :                 +- ReusedExchange (61)
               +- ReusedExchange (68)


(1) Scan parquet spark_catalog.default.catalog_sales
Output [5]: [cs_bill_customer_sk#1, cs_item_sk#2, cs_quantity#3, cs_list_price#4, cs_sold_date_sk#5]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#5)]
ReadSchema: struct<cs_bill_customer_sk:int,cs_item_sk:int,cs_quantity:int,cs_list_price:decimal(7,2)>

(2) ColumnarToRow [codegen id : 5]
Input [5]: [cs_bill_customer_sk#1, cs_item_sk#2, cs_quantity#3, cs_list_price#4, cs_sold_date_sk#5]

(3) Scan parquet spark_catalog.default.store_sales
Output [2]: [ss_item_sk#6, ss_sold_date_sk#7]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#7)]
PushedFilters: [IsNotNull(ss_item_sk)]
ReadSchema: struct<ss_item_sk:int>

(4) ColumnarToRow [codegen id : 3]
Input [2]: [ss_item_sk#6, ss_sold_date_sk#7]

(5) Filter [codegen id : 3]
Input [2]: [ss_item_sk#6, ss_sold_date_sk#7]
Condition : isnotnull(ss_item_sk#6)

(6) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#8, d_date#9, d_year#10]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [In(d_year, [2000,2001,2002,2003]), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_date:date,d_year:int>

(7) ColumnarToRow [codegen id : 1]
Input [3]: [d_date_sk#8, d_date#9, d_year#10]

(8) Filter [codegen id : 1]
Input [3]: [d_date_sk#8, d_date#9, d_year#10]
Condition : (d_year#10 IN (2000,2001,2002,2003) AND isnotnull(d_date_sk#8))

(9) Project [codegen id : 1]
Output [2]: [d_date_sk#8, d_date#9]
Input [3]: [d_date_sk#8, d_date#9, d_year#10]

(10) BroadcastExchange
Input [2]: [d_date_sk#8, d_date#9]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(11) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_sold_date_sk#7]
Right keys [1]: [d_date_sk#8]
Join type: Inner
Join condition: None

(12) Project [codegen id : 3]
Output [2]: [ss_item_sk#6, d_date#9]
Input [4]: [ss_item_sk#6, ss_sold_date_sk#7, d_date_sk#8, d_date#9]

(13) Scan parquet spark_catalog.default.item
Output [2]: [i_item_sk#11, i_item_desc#12]
Batched: true
Location [not included in comparison]/{warehouse_dir}/item]
PushedFilters: [IsNotNull(i_item_sk)]
ReadSchema: struct<i_item_sk:int,i_item_desc:string>

(14) ColumnarToRow [codegen id : 2]
Input [2]: [i_item_sk#11, i_item_desc#12]

(15) Filter [codegen id : 2]
Input [2]: [i_item_sk#11, i_item_desc#12]
Condition : isnotnull(i_item_sk#11)

(16) BroadcastExchange
Input [2]: [i_item_sk#11, i_item_desc#12]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=2]

(17) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_item_sk#6]
Right keys [1]: [i_item_sk#11]
Join type: Inner
Join condition: None

(18) Project [codegen id : 3]
Output [3]: [d_date#9, i_item_sk#11, substr(i_item_desc#12, 1, 30) AS _groupingexpression#13]
Input [4]: [ss_item_sk#6, d_date#9, i_item_sk#11, i_item_desc#12]

(19) HashAggregate [codegen id : 3]
Input [3]: [d_date#9, i_item_sk#11, _groupingexpression#13]
Keys [3]: [_groupingexpression#13, i_item_sk#11, d_date#9]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#14]
Results [4]: [_groupingexpression#13, i_item_sk#11, d_date#9, count#15]

(20) Exchange
Input [4]: [_groupingexpression#13, i_item_sk#11, d_date#9, count#15]
Arguments: hashpartitioning(_groupingexpression#13, i_item_sk#11, d_date#9, 5), ENSURE_REQUIREMENTS, [plan_id=3]

(21) HashAggregate [codegen id : 4]
Input [4]: [_groupingexpression#13, i_item_sk#11, d_date#9, count#15]
Keys [3]: [_groupingexpression#13, i_item_sk#11, d_date#9]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#16]
Results [2]: [i_item_sk#11 AS item_sk#17, count(1)#16 AS cnt#18]

(22) Filter [codegen id : 4]
Input [2]: [item_sk#17, cnt#18]
Condition : (cnt#18 > 4)

(23) Project [codegen id : 4]
Output [1]: [item_sk#17]
Input [2]: [item_sk#17, cnt#18]

(24) BroadcastExchange
Input [1]: [item_sk#17]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=4]

(25) BroadcastHashJoin [codegen id : 5]
Left keys [1]: [cs_item_sk#2]
Right keys [1]: [item_sk#17]
Join type: LeftSemi
Join condition: None

(26) Project [codegen id : 5]
Output [4]: [cs_bill_customer_sk#1, cs_quantity#3, cs_list_price#4, cs_sold_date_sk#5]
Input [5]: [cs_bill_customer_sk#1, cs_item_sk#2, cs_quantity#3, cs_list_price#4, cs_sold_date_sk#5]

(27) Exchange
Input [4]: [cs_bill_customer_sk#1, cs_quantity#3, cs_list_price#4, cs_sold_date_sk#5]
Arguments: hashpartitioning(cs_bill_customer_sk#1, 5), ENSURE_REQUIREMENTS, [plan_id=5]

(28) Sort [codegen id : 6]
Input [4]: [cs_bill_customer_sk#1, cs_quantity#3, cs_list_price#4, cs_sold_date_sk#5]
Arguments: [cs_bill_customer_sk#1 ASC NULLS FIRST], false, 0

(29) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_customer_sk#19, ss_quantity#20, ss_sales_price#21, ss_sold_date_sk#22]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_customer_sk)]
ReadSchema: struct<ss_customer_sk:int,ss_quantity:int,ss_sales_price:decimal(7,2)>

(30) ColumnarToRow [codegen id : 8]
Input [4]: [ss_customer_sk#19, ss_quantity#20, ss_sales_price#21, ss_sold_date_sk#22]

(31) Filter [codegen id : 8]
Input [4]: [ss_customer_sk#19, ss_quantity#20, ss_sales_price#21, ss_sold_date_sk#22]
Condition : isnotnull(ss_customer_sk#19)

(32) Project [codegen id : 8]
Output [3]: [ss_customer_sk#19, ss_quantity#20, ss_sales_price#21]
Input [4]: [ss_customer_sk#19, ss_quantity#20, ss_sales_price#21, ss_sold_date_sk#22]

(33) Scan parquet spark_catalog.default.customer
Output [1]: [c_customer_sk#23]
Batched: true
Location [not included in comparison]/{warehouse_dir}/customer]
PushedFilters: [IsNotNull(c_customer_sk)]
ReadSchema: struct<c_customer_sk:int>

(34) ColumnarToRow [codegen id : 7]
Input [1]: [c_customer_sk#23]

(35) Filter [codegen id : 7]
Input [1]: [c_customer_sk#23]
Condition : isnotnull(c_customer_sk#23)

(36) BroadcastExchange
Input [1]: [c_customer_sk#23]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=6]

(37) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [ss_customer_sk#19]
Right keys [1]: [c_customer_sk#23]
Join type: Inner
Join condition: None

(38) Project [codegen id : 8]
Output [3]: [ss_quantity#20, ss_sales_price#21, c_customer_sk#23]
Input [4]: [ss_customer_sk#19, ss_quantity#20, ss_sales_price#21, c_customer_sk#23]

(39) HashAggregate [codegen id : 8]
Input [3]: [ss_quantity#20, ss_sales_price#21, c_customer_sk#23]
Keys [1]: [c_customer_sk#23]
Functions [1]: [partial_sum((cast(ss_quantity#20 as decimal(10,0)) * ss_sales_price#21))]
Aggregate Attributes [2]: [sum#24, isEmpty#25]
Results [3]: [c_customer_sk#23, sum#26, isEmpty#27]

(40) Exchange
Input [3]: [c_customer_sk#23, sum#26, isEmpty#27]
Arguments: hashpartitioning(c_customer_sk#23, 5), ENSURE_REQUIREMENTS, [plan_id=7]

(41) HashAggregate [codegen id : 9]
Input [3]: [c_customer_sk#23, sum#26, isEmpty#27]
Keys [1]: [c_customer_sk#23]
Functions [1]: [sum((cast(ss_quantity#20 as decimal(10,0)) * ss_sales_price#21))]
Aggregate Attributes [1]: [sum((cast(ss_quantity#20 as decimal(10,0)) * ss_sales_price#21))#28]
Results [2]: [c_customer_sk#23, sum((cast(ss_quantity#20 as decimal(10,0)) * ss_sales_price#21))#28 AS ssales#29]

(42) Filter [codegen id : 9]
Input [2]: [c_customer_sk#23, ssales#29]
Condition : (isnotnull(ssales#29) AND (cast(ssales#29 as decimal(38,8)) > (0.500000 * Subquery scalar-subquery#30, [id=#31])))

(43) Project [codegen id : 9]
Output [1]: [c_customer_sk#23]
Input [2]: [c_customer_sk#23, ssales#29]

(44) Sort [codegen id : 9]
Input [1]: [c_customer_sk#23]
Arguments: [c_customer_sk#23 ASC NULLS FIRST], false, 0

(45) SortMergeJoin [codegen id : 11]
Left keys [1]: [cs_bill_customer_sk#1]
Right keys [1]: [c_customer_sk#23]
Join type: LeftSemi
Join condition: None

(46) Project [codegen id : 11]
Output [3]: [cs_quantity#3, cs_list_price#4, cs_sold_date_sk#5]
Input [4]: [cs_bill_customer_sk#1, cs_quantity#3, cs_list_price#4, cs_sold_date_sk#5]

(47) Scan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#32, d_year#33, d_moy#34]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), EqualTo(d_year,2000), EqualTo(d_moy,2), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>

(48) ColumnarToRow [codegen id : 10]
Input [3]: [d_date_sk#32, d_year#33, d_moy#34]

(49) Filter [codegen id : 10]
Input [3]: [d_date_sk#32, d_year#33, d_moy#34]
Condition : ((((isnotnull(d_year#33) AND isnotnull(d_moy#34)) AND (d_year#33 = 2000)) AND (d_moy#34 = 2)) AND isnotnull(d_date_sk#32))

(50) Project [codegen id : 10]
Output [1]: [d_date_sk#32]
Input [3]: [d_date_sk#32, d_year#33, d_moy#34]

(51) BroadcastExchange
Input [1]: [d_date_sk#32]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8]

(52) BroadcastHashJoin [codegen id : 11]
Left keys [1]: [cs_sold_date_sk#5]
Right keys [1]: [d_date_sk#32]
Join type: Inner
Join condition: None

(53) Project [codegen id : 11]
Output [1]: [(cast(cs_quantity#3 as decimal(10,0)) * cs_list_price#4) AS sales#35]
Input [4]: [cs_quantity#3, cs_list_price#4, cs_sold_date_sk#5, d_date_sk#32]

(54) Scan parquet spark_catalog.default.web_sales
Output [5]: [ws_item_sk#36, ws_bill_customer_sk#37, ws_quantity#38, ws_list_price#39, ws_sold_date_sk#40]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ws_sold_date_sk#40)]
ReadSchema: struct<ws_item_sk:int,ws_bill_customer_sk:int,ws_quantity:int,ws_list_price:decimal(7,2)>

(55) ColumnarToRow [codegen id : 16]
Input [5]: [ws_item_sk#36, ws_bill_customer_sk#37, ws_quantity#38, ws_list_price#39, ws_sold_date_sk#40]

(56) ReusedExchange [Reuses operator id: 24]
Output [1]: [item_sk#17]

(57) BroadcastHashJoin [codegen id : 16]
Left keys [1]: [ws_item_sk#36]
Right keys [1]: [item_sk#17]
Join type: LeftSemi
Join condition: None

(58) Project [codegen id : 16]
Output [4]: [ws_bill_customer_sk#37, ws_quantity#38, ws_list_price#39, ws_sold_date_sk#40]
Input [5]: [ws_item_sk#36, ws_bill_customer_sk#37, ws_quantity#38, ws_list_price#39, ws_sold_date_sk#40]

(59) Exchange
Input [4]: [ws_bill_customer_sk#37, ws_quantity#38, ws_list_price#39, ws_sold_date_sk#40]
Arguments: hashpartitioning(ws_bill_customer_sk#37, 5), ENSURE_REQUIREMENTS, [plan_id=9]

(60) Sort [codegen id : 17]
Input [4]: [ws_bill_customer_sk#37, ws_quantity#38, ws_list_price#39, ws_sold_date_sk#40]
Arguments: [ws_bill_customer_sk#37 ASC NULLS FIRST], false, 0

(61) ReusedExchange [Reuses operator id: 40]
Output [3]: [c_customer_sk#23, sum#26, isEmpty#27]

(62) HashAggregate [codegen id : 20]
Input [3]: [c_customer_sk#23, sum#26, isEmpty#27]
Keys [1]: [c_customer_sk#23]
Functions [1]: [sum((cast(ss_quantity#20 as decimal(10,0)) * ss_sales_price#21))]
Aggregate Attributes [1]: [sum((cast(ss_quantity#20 as decimal(10,0)) * ss_sales_price#21))#28]
Results [2]: [c_customer_sk#23, sum((cast(ss_quantity#20 as decimal(10,0)) * ss_sales_price#21))#28 AS ssales#29]

(63) Filter [codegen id : 20]
Input [2]: [c_customer_sk#23, ssales#29]
Condition : (isnotnull(ssales#29) AND (cast(ssales#29 as decimal(38,8)) > (0.500000 * ReusedSubquery Subquery scalar-subquery#30, [id=#31])))

(64) Project [codegen id : 20]
Output [1]: [c_customer_sk#23]
Input [2]: [c_customer_sk#23, ssales#29]

(65) Sort [codegen id : 20]
Input [1]: [c_customer_sk#23]
Arguments: [c_customer_sk#23 ASC NULLS FIRST], false, 0

(66) SortMergeJoin [codegen id : 22]
Left keys [1]: [ws_bill_customer_sk#37]
Right keys [1]: [c_customer_sk#23]
Join type: LeftSemi
Join condition: None

(67) Project [codegen id : 22]
Output [3]: [ws_quantity#38, ws_list_price#39, ws_sold_date_sk#40]
Input [4]: [ws_bill_customer_sk#37, ws_quantity#38, ws_list_price#39, ws_sold_date_sk#40]

(68) ReusedExchange [Reuses operator id: 51]
Output [1]: [d_date_sk#41]

(69) BroadcastHashJoin [codegen id : 22]
Left keys [1]: [ws_sold_date_sk#40]
Right keys [1]: [d_date_sk#41]
Join type: Inner
Join condition: None

(70) Project [codegen id : 22]
Output [1]: [(cast(ws_quantity#38 as decimal(10,0)) * ws_list_price#39) AS sales#42]
Input [4]: [ws_quantity#38, ws_list_price#39, ws_sold_date_sk#40, d_date_sk#41]

(71) Union

(72) HashAggregate [codegen id : 23]
Input [1]: [sales#35]
Keys: []
Functions [1]: [partial_sum(sales#35)]
Aggregate Attributes [2]: [sum#43, isEmpty#44]
Results [2]: [sum#45, isEmpty#46]

(73) Exchange
Input [2]: [sum#45, isEmpty#46]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=10]

(74) HashAggregate [codegen id : 24]
Input [2]: [sum#45, isEmpty#46]
Keys: []
Functions [1]: [sum(sales#35)]
Aggregate Attributes [1]: [sum(sales#35)#47]
Results [1]: [sum(sales#35)#47 AS sum(sales)#48]

===== Subqueries =====

Subquery:1 Hosting operator id = 42 Hosting Expression = Subquery scalar-subquery#30, [id=#31]
* HashAggregate (93)
+- Exchange (92)
   +- * HashAggregate (91)
      +- * HashAggregate (90)
         +- Exchange (89)
            +- * HashAggregate (88)
               +- * Project (87)
                  +- * BroadcastHashJoin Inner BuildRight (86)
                     :- * Project (80)
                     :  +- * BroadcastHashJoin Inner BuildRight (79)
                     :     :- * Filter (77)
                     :     :  +- * ColumnarToRow (76)
                     :     :     +- Scan parquet spark_catalog.default.store_sales (75)
                     :     +- ReusedExchange (78)
                     +- BroadcastExchange (85)
                        +- * Project (84)
                           +- * Filter (83)
                              +- * ColumnarToRow (82)
                                 +- Scan parquet spark_catalog.default.date_dim (81)


(75) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_customer_sk#49, ss_quantity#50, ss_sales_price#51, ss_sold_date_sk#52]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#52)]
PushedFilters: [IsNotNull(ss_customer_sk)]
ReadSchema: struct<ss_customer_sk:int,ss_quantity:int,ss_sales_price:decimal(7,2)>

(76) ColumnarToRow [codegen id : 3]
Input [4]: [ss_customer_sk#49, ss_quantity#50, ss_sales_price#51, ss_sold_date_sk#52]

(77) Filter [codegen id : 3]
Input [4]: [ss_customer_sk#49, ss_quantity#50, ss_sales_price#51, ss_sold_date_sk#52]
Condition : isnotnull(ss_customer_sk#49)

(78) ReusedExchange [Reuses operator id: 36]
Output [1]: [c_customer_sk#53]

(79) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_customer_sk#49]
Right keys [1]: [c_customer_sk#53]
Join type: Inner
Join condition: None

(80) Project [codegen id : 3]
Output [4]: [ss_quantity#50, ss_sales_price#51, ss_sold_date_sk#52, c_customer_sk#53]
Input [5]: [ss_customer_sk#49, ss_quantity#50, ss_sales_price#51, ss_sold_date_sk#52, c_customer_sk#53]

(81) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#54, d_year#55]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [In(d_year, [2000,2001,2002,2003]), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_year:int>

(82) ColumnarToRow [codegen id : 2]
Input [2]: [d_date_sk#54, d_year#55]

(83) Filter [codegen id : 2]
Input [2]: [d_date_sk#54, d_year#55]
Condition : (d_year#55 IN (2000,2001,2002,2003) AND isnotnull(d_date_sk#54))

(84) Project [codegen id : 2]
Output [1]: [d_date_sk#54]
Input [2]: [d_date_sk#54, d_year#55]

(85) BroadcastExchange
Input [1]: [d_date_sk#54]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=11]

(86) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_sold_date_sk#52]
Right keys [1]: [d_date_sk#54]
Join type: Inner
Join condition: None

(87) Project [codegen id : 3]
Output [3]: [ss_quantity#50, ss_sales_price#51, c_customer_sk#53]
Input [5]: [ss_quantity#50, ss_sales_price#51, ss_sold_date_sk#52, c_customer_sk#53, d_date_sk#54]

(88) HashAggregate [codegen id : 3]
Input [3]: [ss_quantity#50, ss_sales_price#51, c_customer_sk#53]
Keys [1]: [c_customer_sk#53]
Functions [1]: [partial_sum((cast(ss_quantity#50 as decimal(10,0)) * ss_sales_price#51))]
Aggregate Attributes [2]: [sum#56, isEmpty#57]
Results [3]: [c_customer_sk#53, sum#58, isEmpty#59]

(89) Exchange
Input [3]: [c_customer_sk#53, sum#58, isEmpty#59]
Arguments: hashpartitioning(c_customer_sk#53, 5), ENSURE_REQUIREMENTS, [plan_id=12]

(90) HashAggregate [codegen id : 4]
Input [3]: [c_customer_sk#53, sum#58, isEmpty#59]
Keys [1]: [c_customer_sk#53]
Functions [1]: [sum((cast(ss_quantity#50 as decimal(10,0)) * ss_sales_price#51))]
Aggregate Attributes [1]: [sum((cast(ss_quantity#50 as decimal(10,0)) * ss_sales_price#51))#60]
Results [1]: [sum((cast(ss_quantity#50 as decimal(10,0)) * ss_sales_price#51))#60 AS csales#61]

(91) HashAggregate [codegen id : 4]
Input [1]: [csales#61]
Keys: []
Functions [1]: [partial_max(csales#61)]
Aggregate Attributes [1]: [max#62]
Results [1]: [max#63]

(92) Exchange
Input [1]: [max#63]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=13]

(93) HashAggregate [codegen id : 5]
Input [1]: [max#63]
Keys: []
Functions [1]: [max(csales#61)]
Aggregate Attributes [1]: [max(csales#61)#64]
Results [1]: [max(csales#61)#64 AS tpcds_cmax#65]

Subquery:2 Hosting operator id = 63 Hosting Expression = ReusedSubquery Subquery scalar-subquery#30, [id=#31]


