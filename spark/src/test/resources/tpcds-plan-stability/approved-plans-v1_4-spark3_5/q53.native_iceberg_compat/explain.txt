== Physical Plan ==
TakeOrderedAndProject (29)
+- * Project (28)
   +- * Filter (27)
      +- Window (26)
         +- * CometColumnarToRow (25)
            +- CometSort (24)
               +- CometExchange (23)
                  +- CometHashAggregate (22)
                     +- CometExchange (21)
                        +- CometHashAggregate (20)
                           +- CometProject (19)
                              +- CometBroadcastHashJoin (18)
                                 :- CometProject (14)
                                 :  +- CometBroadcastHashJoin (13)
                                 :     :- CometProject (8)
                                 :     :  +- CometBroadcastHashJoin (7)
                                 :     :     :- CometProject (3)
                                 :     :     :  +- CometFilter (2)
                                 :     :     :     +- CometScan [native_iceberg_compat] parquet spark_catalog.default.item (1)
                                 :     :     +- CometBroadcastExchange (6)
                                 :     :        +- CometFilter (5)
                                 :     :           +- CometScan [native_iceberg_compat] parquet spark_catalog.default.store_sales (4)
                                 :     +- CometBroadcastExchange (12)
                                 :        +- CometProject (11)
                                 :           +- CometFilter (10)
                                 :              +- CometScan [native_iceberg_compat] parquet spark_catalog.default.date_dim (9)
                                 +- CometBroadcastExchange (17)
                                    +- CometFilter (16)
                                       +- CometScan [native_iceberg_compat] parquet spark_catalog.default.store (15)


(1) CometScan [native_iceberg_compat] parquet spark_catalog.default.item
Output [5]: [i_item_sk#1, i_brand#2, i_class#3, i_category#4, i_manufact_id#5]
Batched: true
Location [not included in comparison]/{warehouse_dir}/item]
PushedFilters: [IsNotNull(i_item_sk)]
ReadSchema: struct<i_item_sk:int,i_brand:string,i_class:string,i_category:string,i_manufact_id:int>

(2) CometFilter
Input [5]: [i_item_sk#1, i_brand#2, i_class#3, i_category#4, i_manufact_id#5]
Condition : ((((staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, i_category#4, 50, true, false, true) IN (Books                                             ,Children                                          ,Electronics                                       ) AND staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, i_class#3, 50, true, false, true) IN (personal                                          ,portable                                          ,reference                                         ,self-help                                         )) AND staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, i_brand#2, 50, true, false, true) IN (scholaramalgamalg #6                             ,scholaramalgamalg #7                              ,exportiunivamalg #8                               ,scholaramalgamalg #8                              )) OR ((staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, i_category#4, 50, true, false, true) IN (Women                                             ,Music                                             ,Men                                               ) AND staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, i_class#3, 50, true, false, true) IN (accessories                                       ,classical                                         ,fragrances                                        ,pants                                             )) AND staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, i_brand#2, 50, true, false, true) IN (amalgimporto #9                                   ,edu packscholar #9                                ,exportiimporto #9                                 ,importoamalg #9                                   ))) AND isnotnull(i_item_sk#1))

(3) CometProject
Input [5]: [i_item_sk#1, i_brand#2, i_class#3, i_category#4, i_manufact_id#5]
Arguments: [i_item_sk#1, i_manufact_id#5], [i_item_sk#1, i_manufact_id#5]

(4) CometScan [native_iceberg_compat] parquet spark_catalog.default.store_sales
Output [4]: [ss_item_sk#10, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#13), dynamicpruningexpression(ss_sold_date_sk#13 IN dynamicpruning#14)]
PushedFilters: [IsNotNull(ss_item_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_item_sk:int,ss_store_sk:int,ss_sales_price:decimal(7,2)>

(5) CometFilter
Input [4]: [ss_item_sk#10, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13]
Condition : (isnotnull(ss_item_sk#10) AND isnotnull(ss_store_sk#11))

(6) CometBroadcastExchange
Input [4]: [ss_item_sk#10, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13]
Arguments: [ss_item_sk#10, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13]

(7) CometBroadcastHashJoin
Left output [2]: [i_item_sk#1, i_manufact_id#5]
Right output [4]: [ss_item_sk#10, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13]
Arguments: [i_item_sk#1], [ss_item_sk#10], Inner, BuildRight

(8) CometProject
Input [6]: [i_item_sk#1, i_manufact_id#5, ss_item_sk#10, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13]
Arguments: [i_manufact_id#5, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13], [i_manufact_id#5, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13]

(9) CometScan [native_iceberg_compat] parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#15, d_month_seq#16, d_qoy#17]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [In(d_month_seq, [1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211]), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_month_seq:int,d_qoy:int>

(10) CometFilter
Input [3]: [d_date_sk#15, d_month_seq#16, d_qoy#17]
Condition : (d_month_seq#16 INSET 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211 AND isnotnull(d_date_sk#15))

(11) CometProject
Input [3]: [d_date_sk#15, d_month_seq#16, d_qoy#17]
Arguments: [d_date_sk#15, d_qoy#17], [d_date_sk#15, d_qoy#17]

(12) CometBroadcastExchange
Input [2]: [d_date_sk#15, d_qoy#17]
Arguments: [d_date_sk#15, d_qoy#17]

(13) CometBroadcastHashJoin
Left output [4]: [i_manufact_id#5, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13]
Right output [2]: [d_date_sk#15, d_qoy#17]
Arguments: [ss_sold_date_sk#13], [d_date_sk#15], Inner, BuildRight

(14) CometProject
Input [6]: [i_manufact_id#5, ss_store_sk#11, ss_sales_price#12, ss_sold_date_sk#13, d_date_sk#15, d_qoy#17]
Arguments: [i_manufact_id#5, ss_store_sk#11, ss_sales_price#12, d_qoy#17], [i_manufact_id#5, ss_store_sk#11, ss_sales_price#12, d_qoy#17]

(15) CometScan [native_iceberg_compat] parquet spark_catalog.default.store
Output [1]: [s_store_sk#18]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_store_sk)]
ReadSchema: struct<s_store_sk:int>

(16) CometFilter
Input [1]: [s_store_sk#18]
Condition : isnotnull(s_store_sk#18)

(17) CometBroadcastExchange
Input [1]: [s_store_sk#18]
Arguments: [s_store_sk#18]

(18) CometBroadcastHashJoin
Left output [4]: [i_manufact_id#5, ss_store_sk#11, ss_sales_price#12, d_qoy#17]
Right output [1]: [s_store_sk#18]
Arguments: [ss_store_sk#11], [s_store_sk#18], Inner, BuildRight

(19) CometProject
Input [5]: [i_manufact_id#5, ss_store_sk#11, ss_sales_price#12, d_qoy#17, s_store_sk#18]
Arguments: [i_manufact_id#5, ss_sales_price#12, d_qoy#17], [i_manufact_id#5, ss_sales_price#12, d_qoy#17]

(20) CometHashAggregate
Input [3]: [i_manufact_id#5, ss_sales_price#12, d_qoy#17]
Keys [2]: [i_manufact_id#5, d_qoy#17]
Functions [1]: [partial_sum(UnscaledValue(ss_sales_price#12))]

(21) CometExchange
Input [3]: [i_manufact_id#5, d_qoy#17, sum#19]
Arguments: hashpartitioning(i_manufact_id#5, d_qoy#17, 5), ENSURE_REQUIREMENTS, CometNativeShuffle, [plan_id=1]

(22) CometHashAggregate
Input [3]: [i_manufact_id#5, d_qoy#17, sum#19]
Keys [2]: [i_manufact_id#5, d_qoy#17]
Functions [1]: [sum(UnscaledValue(ss_sales_price#12))]

(23) CometExchange
Input [3]: [i_manufact_id#5, sum_sales#20, _w0#21]
Arguments: hashpartitioning(i_manufact_id#5, 5), ENSURE_REQUIREMENTS, CometNativeShuffle, [plan_id=2]

(24) CometSort
Input [3]: [i_manufact_id#5, sum_sales#20, _w0#21]
Arguments: [i_manufact_id#5, sum_sales#20, _w0#21], [i_manufact_id#5 ASC NULLS FIRST]

(25) CometColumnarToRow [codegen id : 1]
Input [3]: [i_manufact_id#5, sum_sales#20, _w0#21]

(26) Window
Input [3]: [i_manufact_id#5, sum_sales#20, _w0#21]
Arguments: [avg(_w0#21) windowspecdefinition(i_manufact_id#5, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS avg_quarterly_sales#22], [i_manufact_id#5]

(27) Filter [codegen id : 2]
Input [4]: [i_manufact_id#5, sum_sales#20, _w0#21, avg_quarterly_sales#22]
Condition : CASE WHEN (avg_quarterly_sales#22 > 0.000000) THEN ((abs((sum_sales#20 - avg_quarterly_sales#22)) / avg_quarterly_sales#22) > 0.1000000000000000) ELSE false END

(28) Project [codegen id : 2]
Output [3]: [i_manufact_id#5, sum_sales#20, avg_quarterly_sales#22]
Input [4]: [i_manufact_id#5, sum_sales#20, _w0#21, avg_quarterly_sales#22]

(29) TakeOrderedAndProject
Input [3]: [i_manufact_id#5, sum_sales#20, avg_quarterly_sales#22]
Arguments: 100, [avg_quarterly_sales#22 ASC NULLS FIRST, sum_sales#20 ASC NULLS FIRST, i_manufact_id#5 ASC NULLS FIRST], [i_manufact_id#5, sum_sales#20, avg_quarterly_sales#22]

===== Subqueries =====

Subquery:1 Hosting operator id = 4 Hosting Expression = ss_sold_date_sk#13 IN dynamicpruning#14
BroadcastExchange (34)
+- * CometColumnarToRow (33)
   +- CometProject (32)
      +- CometFilter (31)
         +- CometScan [native_iceberg_compat] parquet spark_catalog.default.date_dim (30)


(30) CometScan [native_iceberg_compat] parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#15, d_month_seq#16, d_qoy#17]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [In(d_month_seq, [1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211]), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_month_seq:int,d_qoy:int>

(31) CometFilter
Input [3]: [d_date_sk#15, d_month_seq#16, d_qoy#17]
Condition : (d_month_seq#16 INSET 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211 AND isnotnull(d_date_sk#15))

(32) CometProject
Input [3]: [d_date_sk#15, d_month_seq#16, d_qoy#17]
Arguments: [d_date_sk#15, d_qoy#17], [d_date_sk#15, d_qoy#17]

(33) CometColumnarToRow [codegen id : 1]
Input [2]: [d_date_sk#15, d_qoy#17]

(34) BroadcastExchange
Input [2]: [d_date_sk#15, d_qoy#17]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=3]


