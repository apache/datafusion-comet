== Physical Plan ==
* HashAggregate (52)
+- * CometColumnarToRow (51)
   +- CometColumnarExchange (50)
      +- * HashAggregate (49)
         +- * Project (48)
            +- * BroadcastHashJoin LeftAnti BuildRight (47)
               :- * BroadcastHashJoin LeftAnti BuildRight (32)
               :  :- * CometColumnarToRow (17)
               :  :  +- CometHashAggregate (16)
               :  :     +- CometColumnarExchange (15)
               :  :        +- * HashAggregate (14)
               :  :           +- * Project (13)
               :  :              +- * BroadcastHashJoin Inner BuildRight (12)
               :  :                 :- * Project (6)
               :  :                 :  +- * BroadcastHashJoin Inner BuildRight (5)
               :  :                 :     :- * Filter (3)
               :  :                 :     :  +- * ColumnarToRow (2)
               :  :                 :     :     +- Scan parquet spark_catalog.default.store_sales (1)
               :  :                 :     +- ReusedExchange (4)
               :  :                 +- BroadcastExchange (11)
               :  :                    +- * CometColumnarToRow (10)
               :  :                       +- CometProject (9)
               :  :                          +- CometFilter (8)
               :  :                             +- CometNativeScan parquet spark_catalog.default.customer (7)
               :  +- BroadcastExchange (31)
               :     +- * CometColumnarToRow (30)
               :        +- CometHashAggregate (29)
               :           +- CometColumnarExchange (28)
               :              +- * HashAggregate (27)
               :                 +- * Project (26)
               :                    +- * BroadcastHashJoin Inner BuildRight (25)
               :                       :- * Project (23)
               :                       :  +- * BroadcastHashJoin Inner BuildRight (22)
               :                       :     :- * Filter (20)
               :                       :     :  +- * ColumnarToRow (19)
               :                       :     :     +- Scan parquet spark_catalog.default.catalog_sales (18)
               :                       :     +- ReusedExchange (21)
               :                       +- ReusedExchange (24)
               +- BroadcastExchange (46)
                  +- * CometColumnarToRow (45)
                     +- CometHashAggregate (44)
                        +- CometColumnarExchange (43)
                           +- * HashAggregate (42)
                              +- * Project (41)
                                 +- * BroadcastHashJoin Inner BuildRight (40)
                                    :- * Project (38)
                                    :  +- * BroadcastHashJoin Inner BuildRight (37)
                                    :     :- * Filter (35)
                                    :     :  +- * ColumnarToRow (34)
                                    :     :     +- Scan parquet spark_catalog.default.web_sales (33)
                                    :     +- ReusedExchange (36)
                                    +- ReusedExchange (39)


(1) Scan parquet spark_catalog.default.store_sales
Output [2]: [ss_customer_sk#1, ss_sold_date_sk#2]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#2), dynamicpruningexpression(ss_sold_date_sk#2 IN dynamicpruning#3)]
PushedFilters: [IsNotNull(ss_customer_sk)]
ReadSchema: struct<ss_customer_sk:int>

(2) ColumnarToRow [codegen id : 3]
Input [2]: [ss_customer_sk#1, ss_sold_date_sk#2]

(3) Filter [codegen id : 3]
Input [2]: [ss_customer_sk#1, ss_sold_date_sk#2]
Condition : isnotnull(ss_customer_sk#1)

(4) ReusedExchange [Reuses operator id: 57]
Output [2]: [d_date_sk#4, d_date#5]

(5) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_sold_date_sk#2]
Right keys [1]: [d_date_sk#4]
Join type: Inner
Join condition: None

(6) Project [codegen id : 3]
Output [2]: [ss_customer_sk#1, d_date#5]
Input [4]: [ss_customer_sk#1, ss_sold_date_sk#2, d_date_sk#4, d_date#5]

(7) CometNativeScan parquet spark_catalog.default.customer
Output [3]: [c_customer_sk#6, c_first_name#7, c_last_name#8]
Batched: true
Location [not included in comparison]/{warehouse_dir}/customer]
PushedFilters: [IsNotNull(c_customer_sk)]
ReadSchema: struct<c_customer_sk:int,c_first_name:string,c_last_name:string>

(8) CometFilter
Input [3]: [c_customer_sk#6, c_first_name#7, c_last_name#8]
Condition : isnotnull(c_customer_sk#6)

(9) CometProject
Input [3]: [c_customer_sk#6, c_first_name#7, c_last_name#8]
Arguments: [c_customer_sk#6, c_first_name#9, c_last_name#10], [c_customer_sk#6, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, c_first_name#7, 20, true, false, true) AS c_first_name#9, staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, c_last_name#8, 30, true, false, true) AS c_last_name#10]

(10) CometColumnarToRow [codegen id : 2]
Input [3]: [c_customer_sk#6, c_first_name#9, c_last_name#10]

(11) BroadcastExchange
Input [3]: [c_customer_sk#6, c_first_name#9, c_last_name#10]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(12) BroadcastHashJoin [codegen id : 3]
Left keys [1]: [ss_customer_sk#1]
Right keys [1]: [c_customer_sk#6]
Join type: Inner
Join condition: None

(13) Project [codegen id : 3]
Output [3]: [c_last_name#10, c_first_name#9, d_date#5]
Input [5]: [ss_customer_sk#1, d_date#5, c_customer_sk#6, c_first_name#9, c_last_name#10]

(14) HashAggregate [codegen id : 3]
Input [3]: [c_last_name#10, c_first_name#9, d_date#5]
Keys [3]: [c_last_name#10, c_first_name#9, d_date#5]
Functions: []
Aggregate Attributes: []
Results [3]: [c_last_name#10, c_first_name#9, d_date#5]

(15) CometColumnarExchange
Input [3]: [c_last_name#10, c_first_name#9, d_date#5]
Arguments: hashpartitioning(c_last_name#10, c_first_name#9, d_date#5, 5), ENSURE_REQUIREMENTS, CometColumnarShuffle, [plan_id=2]

(16) CometHashAggregate
Input [3]: [c_last_name#10, c_first_name#9, d_date#5]
Keys [3]: [c_last_name#10, c_first_name#9, d_date#5]
Functions: []

(17) CometColumnarToRow [codegen id : 12]
Input [3]: [c_last_name#10, c_first_name#9, d_date#5]

(18) Scan parquet spark_catalog.default.catalog_sales
Output [2]: [cs_bill_customer_sk#11, cs_sold_date_sk#12]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#12), dynamicpruningexpression(cs_sold_date_sk#12 IN dynamicpruning#3)]
PushedFilters: [IsNotNull(cs_bill_customer_sk)]
ReadSchema: struct<cs_bill_customer_sk:int>

(19) ColumnarToRow [codegen id : 6]
Input [2]: [cs_bill_customer_sk#11, cs_sold_date_sk#12]

(20) Filter [codegen id : 6]
Input [2]: [cs_bill_customer_sk#11, cs_sold_date_sk#12]
Condition : isnotnull(cs_bill_customer_sk#11)

(21) ReusedExchange [Reuses operator id: 57]
Output [2]: [d_date_sk#13, d_date#14]

(22) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [cs_sold_date_sk#12]
Right keys [1]: [d_date_sk#13]
Join type: Inner
Join condition: None

(23) Project [codegen id : 6]
Output [2]: [cs_bill_customer_sk#11, d_date#14]
Input [4]: [cs_bill_customer_sk#11, cs_sold_date_sk#12, d_date_sk#13, d_date#14]

(24) ReusedExchange [Reuses operator id: 11]
Output [3]: [c_customer_sk#15, c_first_name#16, c_last_name#17]

(25) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [cs_bill_customer_sk#11]
Right keys [1]: [c_customer_sk#15]
Join type: Inner
Join condition: None

(26) Project [codegen id : 6]
Output [3]: [c_last_name#17, c_first_name#16, d_date#14]
Input [5]: [cs_bill_customer_sk#11, d_date#14, c_customer_sk#15, c_first_name#16, c_last_name#17]

(27) HashAggregate [codegen id : 6]
Input [3]: [c_last_name#17, c_first_name#16, d_date#14]
Keys [3]: [c_last_name#17, c_first_name#16, d_date#14]
Functions: []
Aggregate Attributes: []
Results [3]: [c_last_name#17, c_first_name#16, d_date#14]

(28) CometColumnarExchange
Input [3]: [c_last_name#17, c_first_name#16, d_date#14]
Arguments: hashpartitioning(c_last_name#17, c_first_name#16, d_date#14, 5), ENSURE_REQUIREMENTS, CometColumnarShuffle, [plan_id=3]

(29) CometHashAggregate
Input [3]: [c_last_name#17, c_first_name#16, d_date#14]
Keys [3]: [c_last_name#17, c_first_name#16, d_date#14]
Functions: []

(30) CometColumnarToRow [codegen id : 7]
Input [3]: [c_last_name#17, c_first_name#16, d_date#14]

(31) BroadcastExchange
Input [3]: [c_last_name#17, c_first_name#16, d_date#14]
Arguments: HashedRelationBroadcastMode(List(coalesce(input[0, string, true], ), isnull(input[0, string, true]), coalesce(input[1, string, true], ), isnull(input[1, string, true]), coalesce(input[2, date, true], 1970-01-01), isnull(input[2, date, true])),false), [plan_id=4]

(32) BroadcastHashJoin [codegen id : 12]
Left keys [6]: [coalesce(c_last_name#10, ), isnull(c_last_name#10), coalesce(c_first_name#9, ), isnull(c_first_name#9), coalesce(d_date#5, 1970-01-01), isnull(d_date#5)]
Right keys [6]: [coalesce(c_last_name#17, ), isnull(c_last_name#17), coalesce(c_first_name#16, ), isnull(c_first_name#16), coalesce(d_date#14, 1970-01-01), isnull(d_date#14)]
Join type: LeftAnti
Join condition: None

(33) Scan parquet spark_catalog.default.web_sales
Output [2]: [ws_bill_customer_sk#18, ws_sold_date_sk#19]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ws_sold_date_sk#19), dynamicpruningexpression(ws_sold_date_sk#19 IN dynamicpruning#3)]
PushedFilters: [IsNotNull(ws_bill_customer_sk)]
ReadSchema: struct<ws_bill_customer_sk:int>

(34) ColumnarToRow [codegen id : 10]
Input [2]: [ws_bill_customer_sk#18, ws_sold_date_sk#19]

(35) Filter [codegen id : 10]
Input [2]: [ws_bill_customer_sk#18, ws_sold_date_sk#19]
Condition : isnotnull(ws_bill_customer_sk#18)

(36) ReusedExchange [Reuses operator id: 57]
Output [2]: [d_date_sk#20, d_date#21]

(37) BroadcastHashJoin [codegen id : 10]
Left keys [1]: [ws_sold_date_sk#19]
Right keys [1]: [d_date_sk#20]
Join type: Inner
Join condition: None

(38) Project [codegen id : 10]
Output [2]: [ws_bill_customer_sk#18, d_date#21]
Input [4]: [ws_bill_customer_sk#18, ws_sold_date_sk#19, d_date_sk#20, d_date#21]

(39) ReusedExchange [Reuses operator id: 11]
Output [3]: [c_customer_sk#22, c_first_name#23, c_last_name#24]

(40) BroadcastHashJoin [codegen id : 10]
Left keys [1]: [ws_bill_customer_sk#18]
Right keys [1]: [c_customer_sk#22]
Join type: Inner
Join condition: None

(41) Project [codegen id : 10]
Output [3]: [c_last_name#24, c_first_name#23, d_date#21]
Input [5]: [ws_bill_customer_sk#18, d_date#21, c_customer_sk#22, c_first_name#23, c_last_name#24]

(42) HashAggregate [codegen id : 10]
Input [3]: [c_last_name#24, c_first_name#23, d_date#21]
Keys [3]: [c_last_name#24, c_first_name#23, d_date#21]
Functions: []
Aggregate Attributes: []
Results [3]: [c_last_name#24, c_first_name#23, d_date#21]

(43) CometColumnarExchange
Input [3]: [c_last_name#24, c_first_name#23, d_date#21]
Arguments: hashpartitioning(c_last_name#24, c_first_name#23, d_date#21, 5), ENSURE_REQUIREMENTS, CometColumnarShuffle, [plan_id=5]

(44) CometHashAggregate
Input [3]: [c_last_name#24, c_first_name#23, d_date#21]
Keys [3]: [c_last_name#24, c_first_name#23, d_date#21]
Functions: []

(45) CometColumnarToRow [codegen id : 11]
Input [3]: [c_last_name#24, c_first_name#23, d_date#21]

(46) BroadcastExchange
Input [3]: [c_last_name#24, c_first_name#23, d_date#21]
Arguments: HashedRelationBroadcastMode(List(coalesce(input[0, string, true], ), isnull(input[0, string, true]), coalesce(input[1, string, true], ), isnull(input[1, string, true]), coalesce(input[2, date, true], 1970-01-01), isnull(input[2, date, true])),false), [plan_id=6]

(47) BroadcastHashJoin [codegen id : 12]
Left keys [6]: [coalesce(c_last_name#10, ), isnull(c_last_name#10), coalesce(c_first_name#9, ), isnull(c_first_name#9), coalesce(d_date#5, 1970-01-01), isnull(d_date#5)]
Right keys [6]: [coalesce(c_last_name#24, ), isnull(c_last_name#24), coalesce(c_first_name#23, ), isnull(c_first_name#23), coalesce(d_date#21, 1970-01-01), isnull(d_date#21)]
Join type: LeftAnti
Join condition: None

(48) Project [codegen id : 12]
Output: []
Input [3]: [c_last_name#10, c_first_name#9, d_date#5]

(49) HashAggregate [codegen id : 12]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#25]
Results [1]: [count#26]

(50) CometColumnarExchange
Input [1]: [count#26]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, CometColumnarShuffle, [plan_id=7]

(51) CometColumnarToRow [codegen id : 13]
Input [1]: [count#26]

(52) HashAggregate [codegen id : 13]
Input [1]: [count#26]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#27]
Results [1]: [count(1)#27 AS count(1)#28]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#2 IN dynamicpruning#3
BroadcastExchange (57)
+- * CometColumnarToRow (56)
   +- CometProject (55)
      +- CometFilter (54)
         +- CometNativeScan parquet spark_catalog.default.date_dim (53)


(53) CometNativeScan parquet spark_catalog.default.date_dim
Output [3]: [d_date_sk#4, d_date#5, d_month_seq#29]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_date:date,d_month_seq:int>

(54) CometFilter
Input [3]: [d_date_sk#4, d_date#5, d_month_seq#29]
Condition : (((isnotnull(d_month_seq#29) AND (d_month_seq#29 >= 1200)) AND (d_month_seq#29 <= 1211)) AND isnotnull(d_date_sk#4))

(55) CometProject
Input [3]: [d_date_sk#4, d_date#5, d_month_seq#29]
Arguments: [d_date_sk#4, d_date#5], [d_date_sk#4, d_date#5]

(56) CometColumnarToRow [codegen id : 1]
Input [2]: [d_date_sk#4, d_date#5]

(57) BroadcastExchange
Input [2]: [d_date_sk#4, d_date#5]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8]

Subquery:2 Hosting operator id = 18 Hosting Expression = cs_sold_date_sk#12 IN dynamicpruning#3

Subquery:3 Hosting operator id = 33 Hosting Expression = ws_sold_date_sk#19 IN dynamicpruning#3


