== Physical Plan ==
* CometColumnarToRow (24)
+- CometHashAggregate (23)
   +- CometExchange (22)
      +- CometHashAggregate (21)
         +- CometProject (20)
            +- CometSortMergeJoin (19)
               :- CometSort (9)
               :  +- CometHashAggregate (8)
               :     +- CometColumnarExchange (7)
               :        +- * HashAggregate (6)
               :           +- * Project (5)
               :              +- * BroadcastHashJoin Inner BuildRight (4)
               :                 :- * ColumnarToRow (2)
               :                 :  +- Scan parquet spark_catalog.default.store_sales (1)
               :                 +- ReusedExchange (3)
               +- CometSort (18)
                  +- CometHashAggregate (17)
                     +- CometColumnarExchange (16)
                        +- * HashAggregate (15)
                           +- * Project (14)
                              +- * BroadcastHashJoin Inner BuildRight (13)
                                 :- * ColumnarToRow (11)
                                 :  +- Scan parquet spark_catalog.default.catalog_sales (10)
                                 +- ReusedExchange (12)


(1) Scan parquet spark_catalog.default.store_sales
Output [3]: [ss_item_sk#1, ss_customer_sk#2, ss_sold_date_sk#3]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#3), dynamicpruningexpression(ss_sold_date_sk#3 IN dynamicpruning#4)]
ReadSchema: struct<ss_item_sk:int,ss_customer_sk:int>

(2) ColumnarToRow [codegen id : 2]
Input [3]: [ss_item_sk#1, ss_customer_sk#2, ss_sold_date_sk#3]

(3) ReusedExchange [Reuses operator id: 29]
Output [1]: [d_date_sk#5]

(4) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [ss_sold_date_sk#3]
Right keys [1]: [d_date_sk#5]
Join type: Inner
Join condition: None

(5) Project [codegen id : 2]
Output [2]: [ss_item_sk#1, ss_customer_sk#2]
Input [4]: [ss_item_sk#1, ss_customer_sk#2, ss_sold_date_sk#3, d_date_sk#5]

(6) HashAggregate [codegen id : 2]
Input [2]: [ss_item_sk#1, ss_customer_sk#2]
Keys [2]: [ss_customer_sk#2, ss_item_sk#1]
Functions: []
Aggregate Attributes: []
Results [2]: [ss_customer_sk#2, ss_item_sk#1]

(7) CometColumnarExchange
Input [2]: [ss_customer_sk#2, ss_item_sk#1]
Arguments: hashpartitioning(ss_customer_sk#2, ss_item_sk#1, 5), ENSURE_REQUIREMENTS, CometColumnarShuffle, [plan_id=1]

(8) CometHashAggregate
Input [2]: [ss_customer_sk#2, ss_item_sk#1]
Keys [2]: [ss_customer_sk#2, ss_item_sk#1]
Functions: []

(9) CometSort
Input [2]: [customer_sk#6, item_sk#7]
Arguments: [customer_sk#6, item_sk#7], [customer_sk#6 ASC NULLS FIRST, item_sk#7 ASC NULLS FIRST]

(10) Scan parquet spark_catalog.default.catalog_sales
Output [3]: [cs_bill_customer_sk#8, cs_item_sk#9, cs_sold_date_sk#10]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#10), dynamicpruningexpression(cs_sold_date_sk#10 IN dynamicpruning#4)]
ReadSchema: struct<cs_bill_customer_sk:int,cs_item_sk:int>

(11) ColumnarToRow [codegen id : 4]
Input [3]: [cs_bill_customer_sk#8, cs_item_sk#9, cs_sold_date_sk#10]

(12) ReusedExchange [Reuses operator id: 29]
Output [1]: [d_date_sk#11]

(13) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [cs_sold_date_sk#10]
Right keys [1]: [d_date_sk#11]
Join type: Inner
Join condition: None

(14) Project [codegen id : 4]
Output [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Input [4]: [cs_bill_customer_sk#8, cs_item_sk#9, cs_sold_date_sk#10, d_date_sk#11]

(15) HashAggregate [codegen id : 4]
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Keys [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Functions: []
Aggregate Attributes: []
Results [2]: [cs_bill_customer_sk#8, cs_item_sk#9]

(16) CometColumnarExchange
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Arguments: hashpartitioning(cs_bill_customer_sk#8, cs_item_sk#9, 5), ENSURE_REQUIREMENTS, CometColumnarShuffle, [plan_id=2]

(17) CometHashAggregate
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Keys [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Functions: []

(18) CometSort
Input [2]: [customer_sk#12, item_sk#13]
Arguments: [customer_sk#12, item_sk#13], [customer_sk#12 ASC NULLS FIRST, item_sk#13 ASC NULLS FIRST]

(19) CometSortMergeJoin
Left output [2]: [customer_sk#6, item_sk#7]
Right output [2]: [customer_sk#12, item_sk#13]
Arguments: [customer_sk#6, item_sk#7], [customer_sk#12, item_sk#13], FullOuter

(20) CometProject
Input [4]: [customer_sk#6, item_sk#7, customer_sk#12, item_sk#13]
Arguments: [customer_sk#6, customer_sk#12], [customer_sk#6, customer_sk#12]

(21) CometHashAggregate
Input [2]: [customer_sk#6, customer_sk#12]
Keys: []
Functions [3]: [partial_sum(CASE WHEN (isnotnull(customer_sk#6) AND isnull(customer_sk#12)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (isnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (isnotnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END)]

(22) CometExchange
Input [3]: [sum#14, sum#15, sum#16]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, CometNativeShuffle, [plan_id=3]

(23) CometHashAggregate
Input [3]: [sum#14, sum#15, sum#16]
Keys: []
Functions [3]: [sum(CASE WHEN (isnotnull(customer_sk#6) AND isnull(customer_sk#12)) THEN 1 ELSE 0 END), sum(CASE WHEN (isnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END), sum(CASE WHEN (isnotnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END)]

(24) CometColumnarToRow [codegen id : 5]
Input [3]: [store_only#17, catalog_only#18, store_and_catalog#19]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#3 IN dynamicpruning#4
BroadcastExchange (29)
+- * CometColumnarToRow (28)
   +- CometProject (27)
      +- CometFilter (26)
         +- CometNativeScan parquet spark_catalog.default.date_dim (25)


(25) CometNativeScan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#5, d_month_seq#20]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_month_seq:int>

(26) CometFilter
Input [2]: [d_date_sk#5, d_month_seq#20]
Condition : (((isnotnull(d_month_seq#20) AND (d_month_seq#20 >= 1200)) AND (d_month_seq#20 <= 1211)) AND isnotnull(d_date_sk#5))

(27) CometProject
Input [2]: [d_date_sk#5, d_month_seq#20]
Arguments: [d_date_sk#5], [d_date_sk#5]

(28) CometColumnarToRow [codegen id : 1]
Input [1]: [d_date_sk#5]

(29) BroadcastExchange
Input [1]: [d_date_sk#5]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=4]

Subquery:2 Hosting operator id = 10 Hosting Expression = cs_sold_date_sk#10 IN dynamicpruning#4


