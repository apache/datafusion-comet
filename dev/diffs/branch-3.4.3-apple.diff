diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala
index ab7584e768e..2ada8c28842 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala
@@ -27,7 +27,7 @@ import org.apache.spark.sql.catalyst.SchemaPruningTest
 import org.apache.spark.sql.catalyst.expressions.Concat
 import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
 import org.apache.spark.sql.catalyst.plans.logical.Expand
-import org.apache.spark.sql.comet.CometScanExec
+import org.apache.spark.sql.comet.{CometNativeScanExec, CometScanExec}
 import org.apache.spark.sql.execution.FileSourceScanExec
 import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper
 import org.apache.spark.sql.functions._
@@ -869,6 +869,7 @@ abstract class SchemaPruningSuite
       collect(df.queryExecution.executedPlan) {
         case scan: FileSourceScanExec => scan.requiredSchema
         case scan: CometScanExec => scan.requiredSchema
+        case scan: CometNativeScanExec => scan.requiredSchema
       }
     assert(fileSourceScanSchemata.size === expectedSchemaCatalogStrings.size,
       s"Found ${fileSourceScanSchemata.size} file sources in dataframe, " +
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala
index 18123a4d6ec..fbe4c766eee 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala
@@ -17,6 +17,8 @@
 
 package org.apache.spark.sql
 
+import org.apache.comet.CometConf
+
 import org.apache.spark.{SPARK_DOC_ROOT, SparkRuntimeException}
 import org.apache.spark.sql.catalyst.expressions.Cast._
 import org.apache.spark.sql.catalyst.expressions.TryToNumber
@@ -133,29 +135,31 @@ class StringFunctionsSuite extends QueryTest with SharedSparkSession {
   }
 
   test("string regex_replace / regex_extract") {
-    val df = Seq(
-      ("100-200", "(\\d+)-(\\d+)", "300"),
-      ("100-200", "(\\d+)-(\\d+)", "400"),
-      ("100-200", "(\\d+)", "400")).toDF("a", "b", "c")
-
-    checkAnswer(
-      df.select(
-        regexp_replace($"a", "(\\d+)", "num"),
-        regexp_replace($"a", $"b", $"c"),
-        regexp_extract($"a", "(\\d+)-(\\d+)", 1)),
-      Row("num-num", "300", "100") :: Row("num-num", "400", "100") ::
-        Row("num-num", "400-400", "100") :: Nil)
+    withSQLConf(CometConf.COMET_REGEXP_ALLOW_INCOMPATIBLE.key -> "true") {
+      val df = Seq(
+        ("100-200", "(\\d+)-(\\d+)", "300"),
+        ("100-200", "(\\d+)-(\\d+)", "400"),
+        ("100-200", "(\\d+)", "400")).toDF("a", "b", "c")
 
-    // for testing the mutable state of the expression in code gen.
-    // This is a hack way to enable the codegen, thus the codegen is enable by default,
-    // it will still use the interpretProjection if projection followed by a LocalRelation,
-    // hence we add a filter operator.
-    // See the optimizer rule `ConvertToLocalRelation`
-    checkAnswer(
-      df.filter("isnotnull(a)").selectExpr(
-        "regexp_replace(a, b, c)",
-        "regexp_extract(a, b, 1)"),
-      Row("300", "100") :: Row("400", "100") :: Row("400-400", "100") :: Nil)
+      checkAnswer(
+        df.select(
+          regexp_replace($"a", "(\\d+)", "num"),
+          regexp_replace($"a", $"b", $"c"),
+          regexp_extract($"a", "(\\d+)-(\\d+)", 1)),
+        Row("num-num", "300", "100") :: Row("num-num", "400", "100") ::
+          Row("num-num", "400-400", "100") :: Nil)
+
+      // for testing the mutable state of the expression in code gen.
+      // This is a hack way to enable the codegen, thus the codegen is enable by default,
+      // it will still use the interpretProjection if projection followed by a LocalRelation,
+      // hence we add a filter operator.
+      // See the optimizer rule `ConvertToLocalRelation`
+      checkAnswer(
+        df.filter("isnotnull(a)").selectExpr(
+          "regexp_replace(a, b, c)",
+          "regexp_extract(a, b, 1)"),
+        Row("300", "100") :: Row("400", "100") :: Row("400-400", "100") :: Nil)
+    }
   }
 
   test("non-matching optional group") {
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowIndexSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowIndexSuite.scala
index 36492fe936d..583d9225cca 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowIndexSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowIndexSuite.scala
@@ -20,6 +20,7 @@ import java.io.File
 
 import scala.collection.JavaConverters._
 
+import org.apache.comet.CometConf
 import org.apache.hadoop.fs.Path
 import org.apache.parquet.column.ParquetProperties._
 import org.apache.parquet.hadoop.{ParquetFileReader, ParquetOutputFormat}
@@ -173,6 +174,8 @@ class ParquetRowIndexSuite extends QueryTest with SharedSparkSession {
 
   private def testRowIndexGeneration(label: String, conf: RowIndexTestConf): Unit = {
     test (s"$label - ${conf.desc}") {
+      // native_datafusion Parquet scan does not support row index generation.
+      assume(CometConf.COMET_NATIVE_SCAN_IMPL.get() != CometConf.SCAN_NATIVE_DATAFUSION)
       withSQLConf(conf.sqlConfs: _*) {
         withTempPath { path =>
           val rowIndexColName = FileFormat.ROW_INDEX_TEMPORARY_COLUMN_NAME
@@ -298,6 +301,8 @@ class ParquetRowIndexSuite extends QueryTest with SharedSparkSession {
     val conf = RowIndexTestConf(useDataSourceV2 = useDataSourceV2)
 
     test(s"invalid row index column type - ${conf.desc}") {
+      // native_datafusion Parquet scan does not support row index generation.
+      assume(CometConf.COMET_NATIVE_SCAN_IMPL.get() != CometConf.SCAN_NATIVE_DATAFUSION)
       withSQLConf(conf.sqlConfs: _*) {
         withTempPath{ path =>
           val df = spark.range(0, 10, 1, 1).toDF("id")
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala
index 9e598e7a344..cc5224af735 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala
@@ -669,8 +669,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("partition pruning in broadcast hash joins with aliases",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("partition pruning in broadcast hash joins with aliases") {
     Given("alias with simple join condition, using attribute names only")
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       val df = sql(
@@ -761,7 +760,7 @@ abstract class DynamicPartitionPruningSuiteBase
   }
 
   test("partition pruning in broadcast hash joins",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #1737")) {
     Given("disable broadcast pruning and disable subquery duplication")
     withSQLConf(
       SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true",
@@ -996,8 +995,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("different broadcast subqueries with identical children",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("different broadcast subqueries with identical children") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       withTable("fact", "dim") {
         spark.range(100).select(
@@ -1034,8 +1032,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("avoid reordering broadcast join keys to match input hash partitioning",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("avoid reordering broadcast join keys to match input hash partitioning") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "false",
       SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> "-1") {
       withTable("large", "dimTwo", "dimThree") {
@@ -1195,8 +1192,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("Make sure dynamic pruning works on uncorrelated queries",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("Make sure dynamic pruning works on uncorrelated queries") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       val df = sql(
         """
@@ -1225,7 +1221,7 @@ abstract class DynamicPartitionPruningSuiteBase
 
   test("SPARK-32509: Unused Dynamic Pruning filter shouldn't affect " +
     "canonicalization and exchange reuse",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #1737")) {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       withSQLConf(SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> "-1") {
         val df = sql(
@@ -1248,8 +1244,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("Plan broadcast pruning only when the broadcast can be reused",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("Plan broadcast pruning only when the broadcast can be reused") {
     Given("dynamic pruning filter on the build side")
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       val df = sql(
@@ -1290,8 +1285,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-32659: Fix the data issue when pruning DPP on non-atomic type",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-32659: Fix the data issue when pruning DPP on non-atomic type") {
     Seq(NO_CODEGEN, CODEGEN_ONLY).foreach { mode =>
       Seq(true, false).foreach { pruning =>
         withSQLConf(
@@ -1323,8 +1317,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-32817: DPP throws error when the broadcast side is empty",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-32817: DPP throws error when the broadcast side is empty") {
     withSQLConf(
       SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true",
       SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true",
@@ -1436,8 +1429,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-34637: DPP side broadcast query stage is created firstly",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-34637: DPP side broadcast query stage is created firstly") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       val df = sql(
         """ WITH v as (
@@ -1484,8 +1476,7 @@ abstract class DynamicPartitionPruningSuiteBase
     checkAnswer(df, Row(3, 2) :: Row(3, 2) :: Row(3, 2) :: Row(3, 2) :: Nil)
   }
 
-  test("SPARK-36444: Remove OptimizeSubqueries from batch of PartitionPruning",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-36444: Remove OptimizeSubqueries from batch of PartitionPruning") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
       val df = sql(
         """
@@ -1500,7 +1491,7 @@ abstract class DynamicPartitionPruningSuiteBase
   }
 
   test("SPARK-38148: Do not add dynamic partition pruning if there exists static partition " +
-    "pruning", IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+    "pruning") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
       Seq(
         "f.store_id = 1" -> false,
@@ -1572,8 +1563,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-38674: Remove useless deduplicate in SubqueryBroadcastExec",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-38674: Remove useless deduplicate in SubqueryBroadcastExec") {
     withTable("duplicate_keys") {
       withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
         Seq[(Int, String)]((1, "NL"), (1, "NL"), (3, "US"), (3, "US"), (3, "US"))
@@ -1604,8 +1594,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-39338: Remove dynamic pruning subquery if pruningKey's references is empty",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-39338: Remove dynamic pruning subquery if pruningKey's references is empty") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
       val df = sql(
         """
@@ -1634,8 +1623,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-39217: Makes DPP support the pruning side has Union",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-39217: Makes DPP support the pruning side has Union") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
       val df = sql(
         """
