diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala
index 82f0ef347f8..5fa014f6719 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala
@@ -1101,7 +1101,8 @@ class DataFrameAggregateSuite extends QueryTest
     }
   }
 
-  test("SPARK-32038: NormalizeFloatingNumbers should work on distinct aggregate") {
+  test("SPARK-32038: NormalizeFloatingNumbers should work on distinct aggregate",
+    IgnoreComet("TODO: https://github.com/apache/datafusion-comet/issues/1824")) {
     withTempView("view") {
       val nan1 = java.lang.Float.intBitsToFloat(0x7f800001)
       val nan2 = java.lang.Float.intBitsToFloat(0x7fffffff)
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala
index 31081fea2e9..1f97fe011b5 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameWindowFunctionsSuite.scala
@@ -23,8 +23,9 @@ import org.apache.spark.TestUtils.{assertNotSpilled, assertSpilled}
 import org.apache.spark.sql.catalyst.expressions.{AttributeReference, Expression, Lag, Literal, NonFoldableLiteral}
 import org.apache.spark.sql.catalyst.optimizer.TransposeWindow
 import org.apache.spark.sql.catalyst.plans.physical.HashPartitioning
+import org.apache.spark.sql.comet.execution.shuffle.CometShuffleExchangeExec
 import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper
-import org.apache.spark.sql.execution.exchange.{ENSURE_REQUIREMENTS, Exchange, ShuffleExchangeExec}
+import org.apache.spark.sql.execution.exchange.{ENSURE_REQUIREMENTS, Exchange, ShuffleExchangeExec, ShuffleExchangeLike}
 import org.apache.spark.sql.execution.window.WindowExec
 import org.apache.spark.sql.expressions.{Aggregator, MutableAggregationBuffer, UserDefinedAggregateFunction, Window}
 import org.apache.spark.sql.functions._
@@ -1186,10 +1187,12 @@ class DataFrameWindowFunctionsSuite extends QueryTest
     }
 
     def isShuffleExecByRequirement(
-        plan: ShuffleExchangeExec,
+        plan: ShuffleExchangeLike,
         desiredClusterColumns: Seq[String]): Boolean = plan match {
       case ShuffleExchangeExec(op: HashPartitioning, _, ENSURE_REQUIREMENTS, _) =>
         partitionExpressionsColumns(op.expressions) === desiredClusterColumns
+      case CometShuffleExchangeExec(op: HashPartitioning, _, _, ENSURE_REQUIREMENTS, _, _) =>
+        partitionExpressionsColumns(op.expressions) === desiredClusterColumns
       case _ => false
     }
 
@@ -1212,7 +1215,7 @@ class DataFrameWindowFunctionsSuite extends QueryTest
       val shuffleByRequirement = windowed.queryExecution.executedPlan.exists {
         case w: WindowExec =>
           w.child.exists {
-            case s: ShuffleExchangeExec => isShuffleExecByRequirement(s, Seq("key1", "key2"))
+            case s: ShuffleExchangeLike => isShuffleExecByRequirement(s, Seq("key1", "key2"))
             case _ => false
           }
         case _ => false
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala
index 9e598e7a344..1925aac8d97 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DynamicPartitionPruningSuite.scala
@@ -669,8 +669,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("partition pruning in broadcast hash joins with aliases",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("partition pruning in broadcast hash joins with aliases") {
     Given("alias with simple join condition, using attribute names only")
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       val df = sql(
@@ -761,7 +760,7 @@ abstract class DynamicPartitionPruningSuiteBase
   }
 
   test("partition pruning in broadcast hash joins",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #1737")) {
     Given("disable broadcast pruning and disable subquery duplication")
     withSQLConf(
       SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true",
@@ -996,8 +995,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("different broadcast subqueries with identical children",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("different broadcast subqueries with identical children") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       withTable("fact", "dim") {
         spark.range(100).select(
@@ -1035,7 +1033,7 @@ abstract class DynamicPartitionPruningSuiteBase
   }
 
   test("avoid reordering broadcast join keys to match input hash partitioning",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+    IgnoreComet("TODO: https://github.com/apache/datafusion-comet/issues/1839")) {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "false",
       SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> "-1") {
       withTable("large", "dimTwo", "dimThree") {
@@ -1195,8 +1193,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("Make sure dynamic pruning works on uncorrelated queries",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("Make sure dynamic pruning works on uncorrelated queries") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       val df = sql(
         """
@@ -1225,7 +1222,7 @@ abstract class DynamicPartitionPruningSuiteBase
 
   test("SPARK-32509: Unused Dynamic Pruning filter shouldn't affect " +
     "canonicalization and exchange reuse",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+    IgnoreComet("TODO: https://github.com/apache/datafusion-comet/issues/1839")) {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       withSQLConf(SQLConf.AUTO_BROADCASTJOIN_THRESHOLD.key -> "-1") {
         val df = sql(
@@ -1248,8 +1245,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("Plan broadcast pruning only when the broadcast can be reused",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("Plan broadcast pruning only when the broadcast can be reused") {
     Given("dynamic pruning filter on the build side")
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       val df = sql(
@@ -1290,8 +1286,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-32659: Fix the data issue when pruning DPP on non-atomic type",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-32659: Fix the data issue when pruning DPP on non-atomic type") {
     Seq(NO_CODEGEN, CODEGEN_ONLY).foreach { mode =>
       Seq(true, false).foreach { pruning =>
         withSQLConf(
@@ -1323,8 +1318,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-32817: DPP throws error when the broadcast side is empty",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-32817: DPP throws error when the broadcast side is empty") {
     withSQLConf(
       SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true",
       SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true",
@@ -1437,7 +1431,7 @@ abstract class DynamicPartitionPruningSuiteBase
   }
 
   test("SPARK-34637: DPP side broadcast query stage is created firstly",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+    IgnoreComet("TODO: https://github.com/apache/datafusion-comet/issues/1839")) {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_REUSE_BROADCAST_ONLY.key -> "true") {
       val df = sql(
         """ WITH v as (
@@ -1484,8 +1478,7 @@ abstract class DynamicPartitionPruningSuiteBase
     checkAnswer(df, Row(3, 2) :: Row(3, 2) :: Row(3, 2) :: Row(3, 2) :: Nil)
   }
 
-  test("SPARK-36444: Remove OptimizeSubqueries from batch of PartitionPruning",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-36444: Remove OptimizeSubqueries from batch of PartitionPruning") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
       val df = sql(
         """
@@ -1500,7 +1493,7 @@ abstract class DynamicPartitionPruningSuiteBase
   }
 
   test("SPARK-38148: Do not add dynamic partition pruning if there exists static partition " +
-    "pruning", IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+    "pruning") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
       Seq(
         "f.store_id = 1" -> false,
@@ -1572,8 +1565,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-38674: Remove useless deduplicate in SubqueryBroadcastExec",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-38674: Remove useless deduplicate in SubqueryBroadcastExec") {
     withTable("duplicate_keys") {
       withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
         Seq[(Int, String)]((1, "NL"), (1, "NL"), (3, "US"), (3, "US"), (3, "US"))
@@ -1604,8 +1596,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-39338: Remove dynamic pruning subquery if pruningKey's references is empty",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-39338: Remove dynamic pruning subquery if pruningKey's references is empty") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
       val df = sql(
         """
@@ -1634,8 +1625,7 @@ abstract class DynamicPartitionPruningSuiteBase
     }
   }
 
-  test("SPARK-39217: Makes DPP support the pruning side has Union",
-    IgnoreComet("TODO: Support SubqueryBroadcastExec in Comet: #242")) {
+  test("SPARK-39217: Makes DPP support the pruning side has Union") {
     withSQLConf(SQLConf.DYNAMIC_PARTITION_PRUNING_ENABLED.key -> "true") {
       val df = sql(
         """
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala
index 23d6587943e..140ed7ccea0 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/FileBasedDataSourceSuite.scala
@@ -33,7 +33,7 @@ import org.apache.spark.sql.TestingUDT.{IntervalUDT, NullData, NullUDT}
 import org.apache.spark.sql.catalyst.expressions.{AttributeReference, GreaterThan, Literal}
 import org.apache.spark.sql.catalyst.expressions.IntegralLiteralTestUtils.{negativeInt, positiveInt}
 import org.apache.spark.sql.catalyst.plans.logical.Filter
-import org.apache.spark.sql.comet.{CometBatchScanExec, CometScanExec, CometSortMergeJoinExec}
+import org.apache.spark.sql.comet.{CometBatchScanExec, CometNativeScanExec, CometScanExec, CometSortMergeJoinExec}
 import org.apache.spark.sql.execution.{FileSourceScanLike, SimpleMode}
 import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper
 import org.apache.spark.sql.execution.datasources.FilePartition
@@ -1105,6 +1105,7 @@ class FileBasedDataSourceSuite extends QueryTest
             case f: FileSourceScanLike => f.dataFilters
             case b: BatchScanExec => b.scan.asInstanceOf[FileScan].dataFilters
             case b: CometScanExec => b.dataFilters
+            case b: CometNativeScanExec => b.dataFilters
             case b: CometBatchScanExec => b.scan.asInstanceOf[FileScan].dataFilters
           }.flatten
           assert(filters.contains(GreaterThan(scan.logicalPlan.output.head, Literal(5L))))
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/IgnoreComet.scala b/sql/core/src/test/scala/org/apache/spark/sql/IgnoreComet.scala
index d3c0fa9d6ab..618f2856908 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/IgnoreComet.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/IgnoreComet.scala
@@ -26,6 +26,9 @@ import org.apache.spark.sql.test.SQLTestUtils
  * Tests with this tag will be ignored when Comet is enabled (e.g., via `ENABLE_COMET`).
  */
 case class IgnoreComet(reason: String) extends Tag("DisableComet")
+case class IgnoreCometNativeIcebergCompat(reason: String) extends Tag("DisableComet")
+case class IgnoreCometNativeDataFusion(reason: String) extends Tag("DisableComet")
+case class IgnoreCometNativeScan(reason: String) extends Tag("DisableComet")
 
 /**
  * Helper trait that disables Comet for all tests regardless of default config values.
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala
index 84639f7c87e..7760b02988e 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/JoinSuite.scala
@@ -1441,6 +1441,7 @@ class JoinSuite extends QueryTest with SharedSparkSession with AdaptiveSparkPlan
             true
           case WholeStageCodegenExec(ColumnarToRowExec(
             InputAdapter(CometProjectExec(_, _, _, _, _: CometHashJoinExec, _)))) => true
+          case _: CometHashJoinExec => true
         }.size === 1)
         checkAnswer(shjCodegenDF, Seq.empty)
 
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala
index 18123a4d6ec..fbe4c766eee 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/StringFunctionsSuite.scala
@@ -17,6 +17,8 @@
 
 package org.apache.spark.sql
 
+import org.apache.comet.CometConf
+
 import org.apache.spark.{SPARK_DOC_ROOT, SparkRuntimeException}
 import org.apache.spark.sql.catalyst.expressions.Cast._
 import org.apache.spark.sql.catalyst.expressions.TryToNumber
@@ -133,29 +135,31 @@ class StringFunctionsSuite extends QueryTest with SharedSparkSession {
   }
 
   test("string regex_replace / regex_extract") {
-    val df = Seq(
-      ("100-200", "(\\d+)-(\\d+)", "300"),
-      ("100-200", "(\\d+)-(\\d+)", "400"),
-      ("100-200", "(\\d+)", "400")).toDF("a", "b", "c")
-
-    checkAnswer(
-      df.select(
-        regexp_replace($"a", "(\\d+)", "num"),
-        regexp_replace($"a", $"b", $"c"),
-        regexp_extract($"a", "(\\d+)-(\\d+)", 1)),
-      Row("num-num", "300", "100") :: Row("num-num", "400", "100") ::
-        Row("num-num", "400-400", "100") :: Nil)
+    withSQLConf(CometConf.COMET_REGEXP_ALLOW_INCOMPATIBLE.key -> "true") {
+      val df = Seq(
+        ("100-200", "(\\d+)-(\\d+)", "300"),
+        ("100-200", "(\\d+)-(\\d+)", "400"),
+        ("100-200", "(\\d+)", "400")).toDF("a", "b", "c")
 
-    // for testing the mutable state of the expression in code gen.
-    // This is a hack way to enable the codegen, thus the codegen is enable by default,
-    // it will still use the interpretProjection if projection followed by a LocalRelation,
-    // hence we add a filter operator.
-    // See the optimizer rule `ConvertToLocalRelation`
-    checkAnswer(
-      df.filter("isnotnull(a)").selectExpr(
-        "regexp_replace(a, b, c)",
-        "regexp_extract(a, b, 1)"),
-      Row("300", "100") :: Row("400", "100") :: Row("400-400", "100") :: Nil)
+      checkAnswer(
+        df.select(
+          regexp_replace($"a", "(\\d+)", "num"),
+          regexp_replace($"a", $"b", $"c"),
+          regexp_extract($"a", "(\\d+)-(\\d+)", 1)),
+        Row("num-num", "300", "100") :: Row("num-num", "400", "100") ::
+          Row("num-num", "400-400", "100") :: Nil)
+
+      // for testing the mutable state of the expression in code gen.
+      // This is a hack way to enable the codegen, thus the codegen is enable by default,
+      // it will still use the interpretProjection if projection followed by a LocalRelation,
+      // hence we add a filter operator.
+      // See the optimizer rule `ConvertToLocalRelation`
+      checkAnswer(
+        df.filter("isnotnull(a)").selectExpr(
+          "regexp_replace(a, b, c)",
+          "regexp_extract(a, b, 1)"),
+        Row("300", "100") :: Row("400", "100") :: Row("400-400", "100") :: Nil)
+    }
   }
 
   test("non-matching optional group") {
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala
index c95932efee5..24118a767f9 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/SubquerySuite.scala
@@ -1545,7 +1545,7 @@ class SubquerySuite extends QueryTest
               _.asInstanceOf[FileScanRDD].filePartitions.forall(
                 _.files.forall(_.urlEncodedPath.contains("p=0"))))
         case WholeStageCodegenExec(ColumnarToRowExec(InputAdapter(
-            fs @ CometScanExec(_, _, _, partitionFilters, _, _, _, _, _, _)))) =>
+            fs @ CometScanExec(_, _, _, _, partitionFilters, _, _, _, _, _, _)))) =>
           partitionFilters.exists(ExecSubqueryExpression.hasSubquery) &&
             fs.inputRDDs().forall(
               _.asInstanceOf[FileScanRDD].filePartitions.forall(
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/SQLWindowFunctionSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/SQLWindowFunctionSuite.scala
index eec396b2e39..bf3f1c769d6 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/SQLWindowFunctionSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/SQLWindowFunctionSuite.scala
@@ -18,7 +18,7 @@
 package org.apache.spark.sql.execution
 
 import org.apache.spark.TestUtils.assertSpilled
-import org.apache.spark.sql.{AnalysisException, QueryTest, Row}
+import org.apache.spark.sql.{AnalysisException, IgnoreComet, QueryTest, Row}
 import org.apache.spark.sql.internal.SQLConf.{WINDOW_EXEC_BUFFER_IN_MEMORY_THRESHOLD, WINDOW_EXEC_BUFFER_SPILL_THRESHOLD}
 import org.apache.spark.sql.test.SharedSparkSession
 
@@ -470,7 +470,7 @@ class SQLWindowFunctionSuite extends QueryTest with SharedSparkSession {
       Row(1, 3, null) :: Row(2, null, 4) :: Nil)
   }
 
-  test("test with low buffer spill threshold") {
+  test("test with low buffer spill threshold", IgnoreComet("Comet does not support spilling")) {
     val nums = sparkContext.parallelize(1 to 10).map(x => (x, x % 2)).toDF("x", "y")
     nums.createOrReplaceTempView("nums")
 
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala
index baae214c6ee..47795bf6c58 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/WholeStageCodegenSuite.scala
@@ -17,7 +17,7 @@
 
 package org.apache.spark.sql.execution
 
-import org.apache.spark.sql.{Dataset, QueryTest, Row, SaveMode}
+import org.apache.spark.sql.{Dataset, IgnoreCometSuite, QueryTest, Row, SaveMode}
 import org.apache.spark.sql.catalyst.expressions.codegen.{ByteCodeStats, CodeAndComment, CodeGenerator}
 import org.apache.spark.sql.comet.{CometHashJoinExec, CometSortMergeJoinExec}
 import org.apache.spark.sql.execution.adaptive.DisableAdaptiveExecutionSuite
@@ -30,7 +30,7 @@ import org.apache.spark.sql.test.SharedSparkSession
 import org.apache.spark.sql.types.{IntegerType, StringType, StructType}
 
 // Disable AQE because the WholeStageCodegenExec is added when running QueryStageExec
-class WholeStageCodegenSuite extends QueryTest with SharedSparkSession
+class WholeStageCodegenSuite extends QueryTest with SharedSparkSession with IgnoreCometSuite
   with DisableAdaptiveExecutionSuite {
 
   import testImplicits._
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala
index ab7584e768e..2ada8c28842 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/SchemaPruningSuite.scala
@@ -27,7 +27,7 @@ import org.apache.spark.sql.catalyst.SchemaPruningTest
 import org.apache.spark.sql.catalyst.expressions.Concat
 import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
 import org.apache.spark.sql.catalyst.plans.logical.Expand
-import org.apache.spark.sql.comet.CometScanExec
+import org.apache.spark.sql.comet.{CometNativeScanExec, CometScanExec}
 import org.apache.spark.sql.execution.FileSourceScanExec
 import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanHelper
 import org.apache.spark.sql.functions._
@@ -869,6 +869,7 @@ abstract class SchemaPruningSuite
       collect(df.queryExecution.executedPlan) {
         case scan: FileSourceScanExec => scan.requiredSchema
         case scan: CometScanExec => scan.requiredSchema
+        case scan: CometNativeScanExec => scan.requiredSchema
       }
     assert(fileSourceScanSchemata.size === expectedSchemaCatalogStrings.size,
       s"Found ${fileSourceScanSchemata.size} file sources in dataframe, " +
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala
index 034f56b6230..81af723b4d0 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetFilterSuite.scala
@@ -1503,7 +1503,8 @@ abstract class ParquetFilterSuite extends QueryTest with ParquetTest with Shared
     }
   }
 
-  test("Filters should be pushed down for vectorized Parquet reader at row group level") {
+  test("Filters should be pushed down for vectorized Parquet reader at row group level",
+    IgnoreCometNativeScan("Native scans do not support the tested accumulator")) {
     import testImplicits._
 
     withSQLConf(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key -> "true",
@@ -2059,7 +2060,8 @@ abstract class ParquetFilterSuite extends QueryTest with ParquetTest with Shared
     }
   }
 
-  test("SPARK-34562: Bloom filter push down") {
+  test("SPARK-34562: Bloom filter push down",
+    IgnoreCometNativeScan("Native scans do not support the tested accumulator")) {
     withTempPath { dir =>
       val path = dir.getCanonicalPath
       spark.range(100).selectExpr("id * 2 AS id")
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala
index c9f48e3bcf5..899c86b29d5 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetQuerySuite.scala
@@ -978,7 +978,8 @@ abstract class ParquetQuerySuite extends QueryTest with ParquetTest with SharedS
     }
   }
 
-  test("SPARK-26677: negated null-safe equality comparison should not filter matched row groups") {
+  test("SPARK-26677: negated null-safe equality comparison should not filter matched row groups",
+    IgnoreCometNativeScan("Native scans had the filter pushed into DF operator, cannot strip")) {
     withAllParquetReaders {
       withTempPath { path =>
         // Repeated values for dictionary encoding.
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowIndexSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowIndexSuite.scala
index 36492fe936d..583d9225cca 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowIndexSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowIndexSuite.scala
@@ -20,6 +20,7 @@ import java.io.File
 
 import scala.collection.JavaConverters._
 
+import org.apache.comet.CometConf
 import org.apache.hadoop.fs.Path
 import org.apache.parquet.column.ParquetProperties._
 import org.apache.parquet.hadoop.{ParquetFileReader, ParquetOutputFormat}
@@ -173,6 +174,8 @@ class ParquetRowIndexSuite extends QueryTest with SharedSparkSession {
 
   private def testRowIndexGeneration(label: String, conf: RowIndexTestConf): Unit = {
     test (s"$label - ${conf.desc}") {
+      // native_datafusion Parquet scan does not support row index generation.
+      assume(CometConf.COMET_NATIVE_SCAN_IMPL.get() != CometConf.SCAN_NATIVE_DATAFUSION)
       withSQLConf(conf.sqlConfs: _*) {
         withTempPath { path =>
           val rowIndexColName = FileFormat.ROW_INDEX_TEMPORARY_COLUMN_NAME
@@ -298,6 +301,8 @@ class ParquetRowIndexSuite extends QueryTest with SharedSparkSession {
     val conf = RowIndexTestConf(useDataSourceV2 = useDataSourceV2)
 
     test(s"invalid row index column type - ${conf.desc}") {
+      // native_datafusion Parquet scan does not support row index generation.
+      assume(CometConf.COMET_NATIVE_SCAN_IMPL.get() != CometConf.SCAN_NATIVE_DATAFUSION)
       withSQLConf(conf.sqlConfs: _*) {
         withTempPath{ path =>
           val df = spark.range(0, 10, 1, 1).toDF("id")
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestUtils.scala b/sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestUtils.scala
index 3415a1efde7..bb7dc06ebfd 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestUtils.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/test/SQLTestUtils.scala
@@ -27,6 +27,7 @@ import scala.concurrent.duration._
 import scala.language.implicitConversions
 import scala.util.control.NonFatal
 
+import org.apache.comet.CometConf
 import org.apache.hadoop.fs.Path
 import org.scalactic.source.Position
 import org.scalatest.{BeforeAndAfterAll, Suite, Tag}
@@ -130,7 +131,22 @@ private[sql] trait SQLTestUtils extends SparkFunSuite with SQLTestUtilsBase with
       if (isCometEnabled && testTags.exists(_.isInstanceOf[IgnoreComet])) {
         ignore(testName + " (disabled when Comet is on)", testTags: _*)(testFun)
       } else {
-        super.test(testName, testTags: _*)(testFun)
+        val cometScanImpl = CometConf.COMET_NATIVE_SCAN_IMPL.get(conf)
+        val isNativeIcebergCompat = cometScanImpl == CometConf.SCAN_NATIVE_ICEBERG_COMPAT
+        val isNativeDataFusion = cometScanImpl == CometConf.SCAN_NATIVE_DATAFUSION
+        if (isCometEnabled && isNativeIcebergCompat &&
+          testTags.exists(_.isInstanceOf[IgnoreCometNativeIcebergCompat])) {
+          ignore(testName + " (disabled for NATIVE_ICEBERG_COMPAT)", testTags: _*)(testFun)
+        } else if (isCometEnabled && isNativeDataFusion &&
+          testTags.exists(_.isInstanceOf[IgnoreCometNativeDataFusion])) {
+          ignore(testName + " (disabled for NATIVE_DATAFUSION)", testTags: _*)(testFun)
+        } else if (isCometEnabled && (isNativeDataFusion || isNativeIcebergCompat) &&
+          testTags.exists(_.isInstanceOf[IgnoreCometNativeScan])) {
+          ignore(testName + " (disabled for NATIVE_DATAFUSION and NATIVE_ICEBERG_COMPAT)",
+            testTags: _*)(testFun)
+        } else {
+          super.test(testName, testTags: _*)(testFun)
+        }
       }
     }
   }
